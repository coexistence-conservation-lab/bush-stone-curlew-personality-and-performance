---
title: "Bush stone-curlew behaviour and performance analyes"
author: "Shoshana Rapley"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
devtools::install_github("pratikunterwegs/atlastools")

pacman::p_load(adehabitatHR, amt, atlastools, beepr, ggmap, ggpubr, glmmTMB, gtools, janitor, lme4, move2, moveHMM, progressr, performance, SDLfilter, segclust2d, sf, solitude, suncalc, survival, survminer, readr, readxl, terra, tidyterra, tidyverse, wildlifeDI)

# Google API key for ggmaps
ggmap::register_google(key = "AIzaSyAH3qnqmxDATEluG8lKt2KtnHKLYJc2WaM")

# Background map MR zones 1 and 2
map_z13 <- get_map(c(144.4380, -37.9000), zoom=13, maptype = "satellite")
map_z14 <- get_map(c(144.4380, -37.9000), zoom=14, maptype = "satellite")
map_z15 <- get_map(c(144.4380, -37.9000), zoom=15, maptype = "satellite")

# Bird list excluding Daisy (returned to captivity) & Star (disease fate)
birds <- c("Briar", "Nutmeg", "Aurora", "Robin",
           "Iona", "Sage", "Koda", "Brook", "Clover",
           "Prem", "Rove", "Valentine", "Marmalade", "Wobbles")
```

# Introduction

This Markdown documents pre-release assays and post-release survival and establishment metrics for translocated bush stone-curlews at Mt Rothwell, Victoria. 

# Post-release performance

## Data Cleaning
Telemetry are stored on Movebank, and were downloaded for the period 24/10/2022 (one day before the first release) to end of deployment. 

```{r movebank data}
# Import data from movebank
data_raw <- readr::read_csv(fs::dir_ls(path = "Input/movebank")) %>%
  clean_names() %>%
  # Time in posix format
  mutate(DateTime = as.POSIXct(study_local_timestamp, 
                                 "%Y-%m-%d %H:%M:%S"),
         date = as.Date(DateTime),
  # Rename columns for SDLfilter
         id = individual_local_identifier,
         lat = location_lat,
         lon = location_long,
         qi = gps_satellite_count) %>%
  # Remove days after end of tracking period - for Marmalade whose GPS not removed
  filter(date < "2024-01-12");  beep()
```

Summary stats of number of tracked days.

```{r stats}
summary <- data_raw %>% group_by(id) %>%
  summarise(days = max(date)- min(date))
```

### Speed/angle-based filtering
We applied speed-based filtering using the custom function by Gunner (2023), where user defined speed-thresholds are refined using isolation forest models as a method of unsupervised anomaly  detection.

Here is the function, from (Gunner (2023))[https://github.com/Richard6195/Dead-reckoning-animal-movements-in-R/blob/main/Gundog.GPS.Filter.R]

```{r Gundog GPS filter}
#############################################Gundog.GPS.Filter#############################################
#richard.m.g@hotmail.com
#https://www.researchgate.net/profile/Richard-Gunner-2/research
#https://orcid.org/0000-0002-2054-9944

####A R function to screen for GPS anomalies###

##Key features##

##Specifically, this script is divided into 2 main components -->

###Component 1 = User defined movement thresholds to identify anomalous fixes###

#There are three user-defined thresholds: i)  [R1] = A threshold value of distance (units in meters) between  two sets of coordinates - the raw fixes the and median filtered equivalent. Locational data above this threshold are deemed outliers
#                                        ii)  [R2] = Erroneous 'spikes'- These are identified by computing the outgoing and incoming speed (units in meters per second) and absolute turning angle (units ranging 0-180°) between every combination of three consecutive GPS fixes. High outgoing and incoming speed coupled with an acute (small) vertex angle indicates erronous fixes ('spikes')
#                                       iii)  [R3] = A threshold value of maximum speed using a user-defined stepping range - defined as the interval between each retained fix: A five-fix stepping range is thus the distance and time difference (s) computed between every fifth fix prior to calculating the resultant speed value (m/s). If this value exceeds the threshold, then in this example, it is the fifth fix that is depicted as the outlier (usually, the stepping range just needs to be left at the default value of 1)

#This means there are 3 important thresholds - i) [R1] Instantaneous distance (m) away from surrounding general fixes (median position)
#                                             - ii) [R2] The tandem thresholds of outgoing and incoming speeds (m/s) and absolute turning angle (0-180°) between 3 consecutive fixes. 
#                                            - iii) [R3] Instantaneous speed threshold (m/s), using a given stepping range

##Unique to this script?##

#Some previous methods for GPS filtering have calculated metrics such as the above (distance moved, speed and turn angles) using various pre-defined static windows (e.g., Bjorneraas et al. 2010) and the user chooses sutibale thresholds to evaluate GPS fix uncertainty. 
#This script is unique in that the window resets per user-defined maximum GPS drop-out. For example, if GPS was scheduled as 1 fix/5 mins, and there was a period of 20 minutes without a fix, and the drop-out threshold was set as 600 s (10 mins), then the window and the various running computations would not overlap either side of the drop-out. This ensures measures of fix inaccuracy are more standardized in time and space.
#Moreover, some GPS devices records fixes in pre-set 'bursts'. For example, every five minutes, there could be a burst of six consecutive fixes at 1 Hz; one fix per second for six seconds
#This script offers four options to pre-process coordinates within each burst:
# i) Retain one median longitude and latitude value per burst 
# ii) Retain one mean longitude and latitude value per burst 
# iii) Only keep the last value per burst
# iv) Do nothing (use this option if GPS does not record in bursts or if the fixes within each burst are not standardised to 1 s apart)

#Component 1 method order:
#1) The script starts by pre-processing burst data if required (see above). Then the user-defined maximum GPS drop-out time (units in seconds) triggers an increment in group number each time the GPS drop-out time is reached/surpassed
#2) Per group number, a user-defined sliding window (number of consecutive fixes) is used to compute a center-aligned running median of the longitude and latitude coordinates (following processing of burst data if required) and then the Haversine distance between each 'current' GPS fix and the equivalent running median value is computed. Values are extended at the tails of the window (fill = "extend).  If the length of the running median window is less than the length of the given group number, then the running median window.length will automatically change to the length of the given group number, unless the group number length is less than 3, in which case, no median fixes are computed.
#3) R1 computed 
#4) R2 computed between every combination of three consecutive GPS fixes per group number
#5) R3 computed (this will be the same as R2's outgoing speed if the stepping range is left at the default value of one). The stepping range resets per unique group number.


###Component 2 = Isolation forest models as a method of unsupervised anomaly detection###

#This script also implements Isolation forest models based on the above metrics of; [R1] Distance from median fixes , [R2] Outgoing/incoming speed and absolute turning angle between 3 consecutive fixes, and [R3] Maximum speed. For explanation and examples, see https://sealavi.github.io/Outliers-in-animal-movement-data/. Shauhin Alavi [https://github.com/sealavi] conceived this idea for use in GPS filtering.
#Isolation forest runs multiple times, once as a multidimensional case including all metrics [R1,R2 & R3], and once with each metric as single dimensions. Essentially, this approach grows random decision trees, and increasingly partitions and isolates data using random threshold values. More isolated observations are identified as anomalous.
#The user supplies a quantile anomaly score (default = 99.5% quantile) The distribution of anomaly scores at or higher than this quantile are deemed outliers for each dimensional case.
#The user can then inspect the resultant data frame of potential outliers determined from the user-defined threshold limits and the Isolation forest approach to evaluate filtering performance

################################################################################
##User-defined inputs -->
############################################################
#TS --> Timestamp (as.POSIXct object) (e.g., '2022-08-06 21:46:12')
#Longitude --> GPS longitude coordinates (decimal format)
#Latitude --> GPS latitude coordinates (decimal format)
#Drop.out --> Maximum GPS drop-out time (units in s) used to compute group numbers from which the various running windows/stepping ranges listed above resets per increment (default value = 300) 
#Window.length --> Sliding window length (number of consecutive fixes) used to compute center-aligned running median values of longitude and latitude values (for the R1 threshold) (default value = 10)
#Burst.method  --> Option to pre-process burst data: "median", "mean", "last", or "none" (default = "last")
#Dist.thresh --> Threshold value (m) for R1
#Angle.speed --> Threshold value (m/s) for both incoming and outgoing speeds between 3 consecutive fixes used in evaluating R2
#Angle.thresh --> The absolute angle vertex between 3 consecutive fixes used in evaluating R2, for this, the 'Ang.vertex' threshold is passed if <= to the 'Angle.thresh' value
#Max.speed --> Threshold value (m/s) for R3 
#Speed.step --> The stepping range between fixes for 'Max.speed' calculation (e.g., if default value of 1 is changed to 5, then speed is computed between every 5 fixes per group number)
#I.F_conf = --> Quantile anomaly score (default = 0.995)
#plot --> If set to TRUE (default), then the following plots are given: 

#plot 1 = 'Thresholds' --> Four histograms showing the density distributions of metrics used in evaluating R1, R2 & R3 thresholds. Dashed ablines denote the 95 (green) and 99 (red) percentile, relative to the user-defined threshold values (blue)
#plot 2 = 'Plot' --> Two side-by-side ggplotly interactive plots of GPS track:
        #[left plot] coloured according to whether fix was determined 'Not anomalous' (no thresholds surpassed), 'Possible outlier' (one or two of R1, R2 & R3 were surpassed), or 'Unanimous outlier' (all thresholds surpassed) --> Based on component 1 user-defined thresholds
        #[right plot] coloured according to whether fix was determined 'Not anomalous' 'Possible outlier' or 'Unanimous outlier' --> Based on component 2 user-defined thresholds Isolation forest models. If quantile anomaly scores from all  metrics considered together are above the 'I.F_conf value', then fix considered 'Unanimous outlier', for single dimensional cases; 'Possible outlier'. Otherwise, 'Not anomalous'.

#If plot = TRUE, then a 3-component list is returned, with 'Thresholds', 'Plot', and 'df' (the latter being the resultant data frame). 
#If plot = FALSE, then just 'df' is returned as a data frame

################################################################################
##Data frame outputs -->
############################################################
#Observation --> The row number number of the original input data (subsequent to filtering e.g., of the burst sequences)
#Timestamp --> Input timestamp
#Time.diff --> Time difference (s)
#Longitude 
#Latitude
#Fix.number --> Consecutive numbers of retained fixes
#Window.group --> Integer value increasing by one for each 'new group' made according to the 'Drop.out' value
#Dist.from.median --> Distance that each fix is a way from the median fix value (calculated according to the 'Window.length' input)
#R1 --> TRUE or FALSE (if TRUE, 'Dist.from.median' >= 'Dist.thresh')
#Ang.vertex --> The absolute vertex angle between 3 fixes (°)
#Outgoing.speed --> Outgoing speed of the 3 fixes (m/s)
#Incoming.speed --> Incoming speed of the 3 fixes (m/s)
#R2 --> TRUE or FALSE (if TRUE, then both 'Outgoing.speed' and 'Incoming.speed' are >= 'Angle.speed', and the 'Ang.vertex' <= 'Angle.thresh')
#Maximum.speed --> Speed (m/s) between fixes of the given user-defined stepping range (the 'Speed.step')
#R3 --> TRUE or FALSE (if TRUE, Maximum.speed >= 'Max.speed')
#Verdict --> 'Not anomalous' (no thresholds of R1, R2, or R3 are reached/surpassed), 'Possible outlier' (one or two thresholds of R1, R2, or R3 are reached/surpassed), 'Unanimous outlier' (all thresholds reached/surpassed)
#I.F_overall_anomaly_score --> Overall Isolation Forest anomaly score from multidimensional case including all metrics in tandem [R1,R2 & R3]
#I.F_anomaly_score_R1 --> Isolation Forest anomaly score just considering single case of R1
#I.F_anomaly_score_R2 --> Isolation Forest anomaly score just considering single case of R2
#I.F_anomaly_score_R3 --> Isolation Forest anomaly score just considering single case of R3
#Verdict_IF --> 'Not anomalous' (anomaly scores, both in single cases, and multidimensional case of R1, R2, and R3, are less than the input quantile ('I.F_conf' - default = 0.995) of there respective distributions), 'Possible outlier' (One or a combination of  R1, R2, or R3 anomaly scores are equal to or greater than the input quantile ('I.F_conf' - default = 0.995) of there respective distributions), 'Unanimous outlier' (the distribution of anomaly scores at or higher than the input quantile ('I.F_conf' - default = 0.995) when considered as the multidimensional case [R1, R2, & R3 considered together]

##Based on the verdicts within the returned data frame (and plots, if plot = TRUE), the user can then decide to filter out the necessary fixes. The 'observation' column is given, if ever resultant columns are required to be merged back into the 'original' df (based on matching row numbers)

#############################################Gundog.GPS.Filter#############################################

###START OF FUNCTION###

Gundog.GPS.Filter = function(TS, Longitude, Latitude, Drop.out = 300, Window.length = 10, Burst.method = "last", Dist.thresh = 50, Max.speed = 1, Speed.step = 1, Angle.speed = 1, Angle.thresh = 30, I.F_conf = 0.995, plot = TRUE){
  
  ################################################################################################################################################  
  
  ###Required packages###
  
  if (!require('zoo')){ install.packages('zoo', dependencies = TRUE, type="source")} ; suppressMessages(require("zoo"))
  if (!require('dplyr')){ install.packages('dplyr', dependencies = TRUE, type="source")} ; suppressMessages(require("dplyr"))
  if (!require('tidyr')){ install.packages('tidyr', dependencies = TRUE, type="source")} ; suppressMessages(require("tidyr"))
  if (!require('ggplot2')){ install.packages('ggplot2', dependencies = TRUE, type="source")} ; suppressMessages(require("ggplot2"))
  if (!require('plotly')){ install.packages('plotly', dependencies = TRUE, type="source")} ; suppressMessages(require("plotly"))
  if (!require('solitude')){ install.packages('solitude', dependencies = TRUE, type="source")} ; suppressMessages(require("solitude"))
  
  #Check that required packages are installed on the system
  areinstaled=data.frame(installed.packages())
  
  if(all(c("zoo","dplyr","tidyr","ggplot2", "plotly", "solitude")%in%areinstaled$Package)==FALSE){
    required_packages=c("zoo","dplyr","tidyr","ggplot2", "plotly", "solitude")
    missing_packages=c("zoo","dplyr","tidyr","ggplot2", "plotly", "solitude")%in%areinstaled$Package
    stop(paste("The following packages are not installed:", required_packages[which(missing_packages==FALSE)], sep = " "))
  }
  
  ###Input argument checking###
  
  options(digits.secs = 3) #Specify the number of decimal places of the fractional seconds to show if relevant   
  is.POSIXct = function(x) inherits(x, "POSIXct") #Function to check variable is of POSIXct type
  
  if(is.POSIXct(TS) == FALSE) { #Ensure TS is of type POSIXct (otherwise terminate function)
    stop("TS must be of type POSIXct", .call = FALSE)
  }
  if(length(unique(TS)) != length(TS)){ #Ensure TS does not contain duplicate timestamps (otherwise terminate function)
    stop("TS must not contain duplicates", .call = FALSE)
  }
  if(min(Longitude, na.rm = TRUE) < -180 || max(Longitude, na.rm = TRUE) > 180) { #Ensure coordinates are in decimal format
    stop("Longitude must be between -180 and 180 degrees", call. = FALSE)
  }
  if(min(Latitude, na.rm = TRUE) < -180 || max(Latitude, na.rm = TRUE) > 180) { #Ensure coordinates are in decimal format
    stop("Latitude must be between -90 and 90 degrees", call. = FALSE)
  }
  if(length(Drop.out) > 1 || is.numeric(Drop.out) == FALSE || Drop.out != round(Drop.out)){ #Ensure a single (whole number) numeric value is supplied
    stop("Drop.out threshold must be one numeric value", .call = FALSE)
  }
  if(length(Window.length) > 1 || is.numeric(Window.length) == FALSE || Window.length != round(Window.length)){ #Ensure a single (whole number) numeric value is supplied
    stop("Window.length threshold must be one numeric value", .call = FALSE)
  }
  if(length(Burst.method) > 1 || Burst.method %in% c("median", "mean", "last", "none") == FALSE ){ #Ensure a valid option is selected
    stop("Pick one of the pre-select options for Burst.method", .call = FALSE)
  }
  if(length(Dist.thresh) > 1 || is.numeric(Dist.thresh) == FALSE){ #Ensure a single numeric value is supplied
    stop("Dist.thresh threshold must be one numeric value", .call = FALSE)
  }  
  if(length(Max.speed) > 1 || is.numeric(Max.speed) == FALSE){ #Ensure a single numeric value is supplied
    stop("Max.speed threshold must be one numeric value", .call = FALSE)
  }
  if(length(Angle.speed) > 1 || is.numeric(Angle.speed) == FALSE){ #Ensure a single numeric value is supplied
    stop("Angle.speed threshold must be one numeric value", .call = FALSE)
  }
  if(length(Angle.thresh) > 1 || is.numeric(Angle.thresh) == FALSE){ #Ensure a single numeric value is supplied
    stop("Angle.thresh threshold must be one numeric value", .call = FALSE)
  }
  
  ################################################################################################################################################
  
  ####Required functions###
  
  #Haversine distance formula 
  disty = function(long1, lat1, long2, lat2) { #Longitude and Latitude supplied in degrees
    long1 = long1 * pi/180 ; long2 = long2 * pi/180 ; lat1 = lat1 * pi/180 ; lat2 = lat2 * pi/180 #Function converts to radians
    a = sin((lat2 - lat1) / 2) * sin((lat2 - lat1) / 2) + cos(lat1) * cos(lat2) * sin((long2 - long1) / 2) * sin((long2 - long1) / 2)
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    d1 = 6378137 * c
    return(d1)
  }
  
  #Bearing function --> returns degrees - Great circular bearing between 2D positions
  beary = function(long1, lat1, long2, lat2) { #Longitude and Latitude supplied in degrees
    long1 = long1 * pi/180 ; long2 = long2 * pi/180 ; lat1 = lat1 * pi/180 ; lat2 = lat2 * pi/180 #Function converts to radians
    a = sin(long2 - long1) * cos(lat2)
    b = cos(lat1) * sin(lat2) - sin(lat1) * cos(lat2) * cos(long2 - long1)
    c = ((atan2(a, b) / pi) * 180)  #Units returned in degrees (-180 to +180 degree scale)
    return(c)
  }
  
  #Set to 5 decimal places (for plotting long/lat coords purposes)
  scaleFUN <- function(x) sprintf("%.5f", x)
  
  ################################################################################################################################################
  
  ###Create the initial data frame and pre-process the data###
  
  Observation = rep(1:length(TS)) #Row number (used for indexing and merging data frames)
  df = data.frame(Observation, TS, Longitude, Latitude) ; colnames(df) = c("Observation", "Timestamp", "Longitude", "Latitude")
  
  #Remove NA fixes
  df$Longitude = ifelse(df$Longitude == 0, NA, df$Longitude) #In case missing coordinates are filled as zeros, replace with NA's
  df$Latitude = ifelse(df$Latitude == 0, NA, df$Latitude) #In case missing coordinates are filled as zeros, replace with NA's
  df = df[!with(df, is.na(Longitude) | is.na(Latitude)) ,]
  
  #Create a time difference (s) between values
  df = df %>% mutate(Time.diff = as.numeric(c(0, difftime(Timestamp, dplyr::lag(Timestamp), units = "secs")[-1]))) %>% ungroup()
  
  #Make a group column which increments by one each GPS burst. This is going to be used to pre-process each burst
  df$Fix.number = 1
  x = 1
  for(i in 1:nrow(df)){
    if(df$Time.diff[i] > 1){
      x = x+1
    }
    df$Fix.number[i] = x
  }
  
  #Method to process each burst
  if(Burst.method != "none"){
    if(Burst.method == "median"){
      df = df %>% group_by(Fix.number) %>% mutate(Latitude = median(Latitude)) %>% ungroup()
      df = df %>% group_by(Fix.number) %>% mutate(Longitude = median(Longitude)) %>% ungroup()
    }
    if(Burst.method == "mean"){
      df = df %>% group_by(Fix.number) %>% mutate(Latitude = mean(Latitude)) %>% ungroup()
      df = df %>% group_by(Fix.number) %>% mutate(Longitude = mean(Longitude)) %>% ungroup()
    }
    if(Burst.method == "last"){
      df = df %>% group_by(Fix.number) %>% mutate(Latitude = last(Latitude)) %>% ungroup()
      df = df %>% group_by(Fix.number) %>% mutate(Longitude = last(Longitude)) %>% ungroup()
    }
  }
  
  #Remove duplicated processed GPS fixes per burst
  if(Burst.method != "none"){
    df.sub = df[!duplicated(df$Fix.number), ]
    df.sub = df.sub %>% mutate(Time.diff = as.numeric(c(0, difftime(Timestamp, dplyr::lag(Timestamp), units = "secs")[-1]))) %>% ungroup() #Redo time difference between rows after subset
  }else{ 
    df.sub = df
  }
  
  #Make a group column ('Window.group') which increments by one each time the GPS drops-out (missing fixes) >= the user-defined 'Drop.out' threshold (s)
  x = 1
  Time.diff = df.sub$Time.diff
  Window.group = rep(0, length(Time.diff))
  for (i in 1:length(Window.group)) {
    if(Time.diff[i] >= Drop.out){
      x = x+1
    }
    Window.group[i] = x
  }
  df.sub$Window.group = Window.group
  
  ################################################################################################################################################  
  
  #User-defined thresholds component of script
  
  ###Calculate distance between raw fixes and the median equivalent --> R1###
  
  #Median Longitude and Latitude values per Window.group. If the length of the Window.group < Window.length, then the window.length will change to the length of the Window.group, unless the Window.group < 3, in which case, no median fixes are computed.
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Window.group.length = max(sequence(rle(Window.group)$lengths))) %>% ungroup() #Number of consecutive rows per Window.group
  df.sub$Window.group.length = ifelse(df.sub$Window.group.length >= Window.length, Window.length, df.sub$Window.group.length) #If the number of consecutive rows per Window.group is >= Window.length, then make this value the window.length
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Longitude.med = ifelse(Window.group.length < 3, NA, #If the number of consecutive rows per Window.group is not a minimum of three then the median fix is not calculated. If the number of consecutive rows per Window.group is > 3 and less than the Window.length, then the running median window shortens to this value. Otherwise, the user-defined Window.length is used
                                                                               rollapply(Longitude, align = "center", width = Window.group.length, FUN = median, fill = NA))) %>% tidyr::fill(Longitude.med, .direction = "updown") %>% ungroup() #Replace last Non-NA value per Window.group in both directions
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Latitude.med = ifelse(Window.group.length < 3, NA, #If the number of consecutive rows per Window.group is not a minimum of three then the median fix is not calculated. If the number of consecutive rows per Window.group is > 3 and less than the Window.length, then the running median window shortens to this value. Otherwise, the user-defined Window.length is used
                                                                              rollapply(Latitude, align = "center", width = Window.group.length, FUN = median, fill = NA))) %>% tidyr::fill(Latitude.med, .direction = "updown") %>% ungroup() #Replace last Non-NA value per Window.group in both directions
  
  df.sub$Dist.from.median = with(df.sub, disty(Longitude, Latitude, Longitude.med, Latitude.med)) #Distance between raw fixes and the median equivalent
  
  #Evaluating the threshold
  df.sub$R1 = ifelse(df.sub$Dist.from.median >= Dist.thresh, TRUE, FALSE) #R1 = TRUE if GPS fix identified as erroneous
  
  ###Calculate Erroneous 'spikes' by identifying outgoing and incoming speed (m/s) and absolute turning angle (0-180°) between every combination of three consecutive GPS fixes within the user-defined sliding window length per group --> R2###
  
  #Shift longitude and latitude rows forwards (lag) and backwards (lead) by one row per group
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Longitude.lag = dplyr::lag(Longitude, n = 1, default = NA)) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Latitude.lag = dplyr::lag(Latitude, n = 1, default = NA)) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Longitude.lead = dplyr::lead(Longitude, n = 1, default = NA)) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Latitude.lead = dplyr::lead(Latitude, n = 1, default = NA)) %>% ungroup()
  
  #Calculate bearing (angle) between 'Longitude'/'Latitude', 'Longitude.lag'/'Latitude.lag', and 'Longitude.lead'/'Latitude.lead' (angle from central fix to 'pre-fix' and from central fix to 'post-fix' across the three candidate fixes)
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Ang.lag = beary(Longitude, Latitude, Longitude.lag, Latitude.lag)) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Ang.lead = beary(Longitude, Latitude, Longitude.lead, Latitude.lead)) %>% ungroup()
  df.sub$Ang.lag = ifelse(df.sub$Ang.lag < 0, df.sub$Ang.lag + 360, df.sub$Ang.lag) #Because above formula outputs within the scale of -180 to +180 degrees --> This ensures output is 0 to 360 degrees
  df.sub$Ang.lead = ifelse(df.sub$Ang.lead < 0, df.sub$Ang.lead + 360, df.sub$Ang.lead) #Because above formula outputs within the scale of -180 to +180 degrees --> This ensures output is 0 to 360 degrees
  df.sub$Ang.vertex = df.sub$Ang.lead - df.sub$Ang.lag #Turning angle (0-180°) between every combination of three consecutive GPS fixes
  df.sub$Ang.vertex = ifelse(df.sub$Ang.vertex < -180, (df.sub$Ang.vertex + 360), df.sub$Ang.vertex) #Ensure difference does not exceed 180 degrees in either circular direction
  df.sub$Ang.vertex = ifelse(df.sub$Ang.vertex > 180, (df.sub$Ang.vertex - 360), df.sub$Ang.vertex) #Ensure difference does not exceed 180 degrees in either circular direction
  df.sub$Ang.vertex = abs(df.sub$Ang.vertex) #Make angle absolute
  
  #Calculate the incoming and outgoing speeds either side of angle vertex
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Dist.lag = disty(Longitude, Latitude, Longitude.lag, Latitude.lag)) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Dist.lead = disty(Longitude, Latitude, Longitude.lead, Latitude.lead)) %>% ungroup()
  
  #Lead of time difference
  #Create lag and lead time difference (s) between values
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Time.diff.lag = as.numeric(c(NA, difftime(Timestamp, dplyr::lag(Timestamp), units = "secs")[-1]))) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Time.diff.lead = dplyr::lead(Time.diff.lag, n = 1, default = NA)) %>% ungroup()
  
  #Outgoing and incoming speed (m/s)
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Outgoing.speed = Dist.lag / Time.diff.lag) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Incoming.speed = Dist.lead / Time.diff.lead) %>% ungroup()
  
  #Evaluating the threshold
  df.sub$R2 = ifelse(df.sub$Outgoing.speed >= Angle.speed & df.sub$Incoming.speed >= Angle.speed & df.sub$Ang.vertex <= Angle.thresh, TRUE, FALSE) #R2 = TRUE if GPS fix identified as erroneous
  
  ##Calculate Erroneous fixes if maximum speed threshold calculated using a user-defined stepping range is exceed --> R3###
  
  #Shift longitude and latitude values backwards by the specified stepping range (Speed.step)
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Longitude.lag.2 = dplyr::lag(Longitude, n = Speed.step, default = NA)) %>% ungroup()
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Latitude.lag.2 = dplyr::lag(Latitude, n = Speed.step, default = NA)) %>% ungroup()
  #Calculate time difference between the specified stepping range (Speed.step)
  df.sub = df.sub %>% group_by(Window.group) %>% mutate(Time.diff.lag.2 = as.numeric(c(NA, difftime(Timestamp, dplyr::lag(Timestamp, n = Speed.step, default = NA), units = "secs")[-1]))) %>% ungroup()
  #Calculate the speed across the specified 'Speed.step' stepping range
  df.sub$Maximum.speed = with(df.sub, disty(Longitude, Latitude, Longitude.lag.2, Latitude.lag.2) / Time.diff.lag.2)
  
  #Evaluating the threshold
  df.sub$R3 = ifelse(df.sub$Maximum.speed >= Max.speed, TRUE, FALSE) #R3 = TRUE if GPS fix identified as erroneous
  
  #Replace possible NAs in R1, R2 & R3 to FALSE
  df.sub$R1 = ifelse(is.na(df.sub$R1) == TRUE, FALSE, df.sub$R1)
  df.sub$R2 = ifelse(is.na(df.sub$R2) == TRUE, FALSE, df.sub$R2)
  df.sub$R3 = ifelse(is.na(df.sub$R3) == TRUE, FALSE, df.sub$R3)
  
  #If at least one threshold is passed, then 'Possible outlier', if all thresholds surpassed, then 'Unanimous outlier', if no thresholds surpassed, then 'Not anomalous' 
  df.sub$Verdict = ifelse(df.sub$R1 == TRUE & df.sub$R2 == TRUE & df.sub$R3 == TRUE, "Unanimous outlier", ifelse(df.sub$R1 == TRUE | df.sub$R2 == TRUE | df.sub$R3 == TRUE, "Possible outlier", "Not anomalous"))
  #df.sub$Verdict = as.character(factor(df.sub$Verdict, levels = c("Not anomalous", "Possible outlier", "Unanimous outlier"))) #Ensure factor levels are ordered for plotting purposes
  
  #Remove redundant columns / vectors
  rm(Observation, x, Window.group)
  rm(df)
  df.sub = df.sub[, c('Observation', 'Timestamp', 'Time.diff', 'Longitude', 'Latitude', 'Fix.number', 'Window.group', 'Dist.from.median', 'R1', 'Ang.vertex', 'Outgoing.speed', 'Incoming.speed', 'R2', 'Maximum.speed', 'R3', 'Verdict')]
  
  ################################################################################################################################################
  ################################################################################################################################################  
  
  ###Isolation forest component of function###
  
  ##check if default isolation forest sample size settings need to be adjusted
  if(nrow(df.sub) < 256){
    sample_size = nrow(df.sub)
  }else{
    sample_size = 256
  }
  
  #Begin anomaly detection
  message("Initiating Isolation forest anomaly detection")
  if(all(diff((df.sub$Timestamp)) > 0) == FALSE){
    stop("Timestamps may be out of order - Ensure that timestamps are in ascending order")
  }
  
  #All variables together
  indexs = with(df.sub, which(complete.cases(Dist.from.median, Ang.vertex, Outgoing.speed, Incoming.speed, Maximum.speed) == TRUE)) #Row numbers with complete cases across the columns of interest (non-NA values)
  COLS = c("Dist.from.median", "Ang.vertex", "Outgoing.speed", "Incoming.speed", "Maximum.speed") #Columns of interest
  COLS = which(names(df.sub) %in% COLS == TRUE) #Index of where columns occur in 'df.sub'
  subset = df.sub[indexs, COLS] #Subset just columns of interest
  iforest = isolationForest$new(sample_size = sample_size)
  invisible(capture.output(iforest$fit(subset)))
  scores = iforest$predict(subset)
  df.sub$I.F_overall_anomaly_score = NA
  df.sub$I.F_overall_anomaly_score[indexs] = scores$anomaly_score
  
  #Just 'Dist.from.median' [R1]
  indexs = with(df.sub, which(complete.cases(Dist.from.median) == TRUE)) #Row numbers with non-NA values
  subset = as.data.frame(df.sub$Dist.from.median[indexs]) #Subset just column of interest
  iforest = isolationForest$new(sample_size = sample_size)
  invisible(capture.output(iforest$fit(subset)))
  scores = iforest$predict(subset)
  df.sub$I.F_anomaly_score_R1 = NA
  df.sub$I.F_anomaly_score_R1[indexs] = scores$anomaly_score
  
  #Just 'Ang.vertex', 'Outgoing.speed' and 'Incoming.speed' [R2]
  indexs = with(df.sub, which(complete.cases(Ang.vertex, Outgoing.speed, Incoming.speed) == TRUE)) #Row numbers with complete cases across the columns of interest (non-NA values)
  COLS = c("Ang.vertex", "Outgoing.speed", "Incoming.speed") #Columns of interest
  COLS = which(names(df.sub) %in% COLS == TRUE) #Index of where columns occur in 'df.sub'
  subset = df.sub[indexs, COLS] #Subset just columns of interest
  iforest = isolationForest$new(sample_size = sample_size)
  invisible(capture.output(iforest$fit(subset)))
  scores = iforest$predict(subset)
  df.sub$I.F_anomaly_score_R2 = NA
  df.sub$I.F_anomaly_score_R2[indexs] = scores$anomaly_score
  
  #Just 'Maximum.speed' [R3]
  indexs = with(df.sub, which(complete.cases(Maximum.speed) == TRUE)) #Row numbers with non-NA values
  subset = as.data.frame(df.sub$Maximum.speed[indexs]) #Subset just column of interest
  iforest = isolationForest$new(sample_size = sample_size)
  invisible(capture.output(iforest$fit(subset)))
  scores = iforest$predict(subset)
  df.sub$I.F_anomaly_score_R3 = NA
  df.sub$I.F_anomaly_score_R3[indexs] = scores$anomaly_score
  
  #Which rows correspond to being >= I.F_conf quantile (for R1, R2, R3, and combined)
  R1_outliers = which(df.sub$I.F_anomaly_score_R1 >= as.numeric(quantile(df.sub$I.F_anomaly_score_R1, I.F_conf, na.rm = TRUE)))
  R2_outliers = which(df.sub$I.F_anomaly_score_R2 >= as.numeric(quantile(df.sub$I.F_anomaly_score_R2, I.F_conf, na.rm = TRUE)))
  R3_outliers = which(df.sub$I.F_anomaly_score_R3 >= as.numeric(quantile(df.sub$I.F_anomaly_score_R3, I.F_conf, na.rm = TRUE)))
  overall_outliers = which(df.sub$I.F_overall_anomaly_score >= as.numeric(quantile(df.sub$I.F_overall_anomaly_score, I.F_conf, na.rm = TRUE)))
  
  R1_outliers = R1_outliers[-which(R1_outliers %in% overall_outliers == TRUE)]
  R2_outliers = R2_outliers[-which(R2_outliers %in% overall_outliers == TRUE)]
  R3_outliers = R3_outliers[-which(R3_outliers %in% overall_outliers == TRUE)]
  
  #Label as "Not anomalous", "Possible outlier", "Unanimous outlier", according to whether none, some or all thresholds were surpassed
  df.sub$Verdict_IF = "Not anomalous"
  df.sub$Verdict_IF[R1_outliers] = "Possible outlier"
  df.sub$Verdict_IF[R2_outliers] = "Possible outlier"
  df.sub$Verdict_IF[R3_outliers] = "Possible outlier"
  df.sub$Verdict_IF[overall_outliers] = "Unanimous outlier"
  
  ###Plot results###
  
  if(plot == TRUE){
    
    #summary plots - 'Thresholds'
    old.par <- par(mar = c(0, 0, 0, 0))
    #(1) Distribution of Distance from median (R1)
    par(old.par)
    par(mfrow = c(2,2))
    h = hist(df.sub$Dist.from.median, breaks="Scott", plot=TRUE, main = "Distance from median (m)",
             xlab = "Distance from median (m) [R1]", ylab = "Density", cex.lab = 1.2,  freq = FALSE)
    text(Dist.thresh, max(h$density), labels = paste("R1 =", Dist.thresh), col = "blue", cex = 0.8) # User defined 'Dist.thresh [R1]'
    text(quantile(df.sub$Dist.from.median, 0.95, na.rm = TRUE), max(h$density)/1.7, labels = "95%", col = "green", cex = 0.8) # 95%
    text(quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE), max(h$density)/3.2, labels = "99%", col = "red", cex = 0.8) # 99%
    clip(x1 = min(h$breaks),
         x2 = max(h$breaks), 
         y1 = min(h$density),
         y2 = max(h$density) - max(h$density)/10)
    abline(v = Dist.thresh, col="blue", lwd=1.5, lty=2) # User defined 'Dist.thresh [R1]'
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(df.sub$Dist.from.median, 0.95, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    #Zoom in
    par(fig = c(0.2, 0.49, 0.65, 0.99), new = T)
    h = hist(df.sub$Dist.from.median, breaks="Scott", plot=TRUE, main = "",
             xlab = "", ylab = "", freq = FALSE, xlim = c(0, quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE)))
    if(Dist.thresh < quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE)){
      text(Dist.thresh, max(h$density), labels = paste("R1 =", Dist.thresh), col = "blue", cex = 0.8) # User defined 'Dist.thresh [R1]'
    }
    text(quantile(df.sub$Dist.from.median, 0.95, na.rm = TRUE), max(h$density)/1.7, labels = "95%", col = "green", cex = 0.8) # 95%
    text(quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE), max(h$density)/3.2, labels = "99%", col = "red", cex = 0.8) # 99%
    if(Dist.thresh < quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE)){
      clip(x1 = 0,
           x2 = max(h$breaks), 
           y1 = 0,
           y2 = max(h$density) - max(h$density)/10)
    }
    abline(v = Dist.thresh, col="blue", lwd=1.5, lty=2) # User defined 'Dist.thresh [R1]'
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(df.sub$Dist.from.median, 0.95, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(df.sub$Dist.from.median, 0.99, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    
    #(2) Distribution of Angle between 3 fixes (R2)
    par(fig = c(0.5, 1, 0.5, 1), new = T)
    h = hist(df.sub$Ang.vertex, breaks = 60, plot=TRUE, main = "Angle between 3 fixes (°)",
             xlab = "Angle between 3 fixes (°) [R2]", ylab = "Density", cex.lab = 1.2,  freq = FALSE, xaxt = "n")
    axis(1, at=seq(0,180,by=20), labels=seq(0,180,by=20))
    text(Angle.thresh, max(h$density), labels = paste("R2 =", Angle.thresh), col = "blue", cex = 0.8) # User defined 'Angle.thresh [R1]'
    text(quantile(df.sub$Ang.vertex, 0.05, na.rm = TRUE), max(h$density)/1.7, labels = "0.5%", col = "green", cex = 0.8) # 0.5%
    text(quantile(df.sub$Ang.vertex, 0.01, na.rm = TRUE), max(h$density)/3.2, labels = "0.01%", col = "red", cex = 0.8) # 0.01%
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density) - max(h$density)/10)
    abline(v = Angle.thresh, col="blue", lwd=1.5, lty=2) # User defined 'Angle.thresh [R2]'
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(df.sub$Ang.vertex, 0.05, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(df.sub$Ang.vertex, 0.01, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    
    #(3) Distribution of Outgoing/Incoming speed (R2)
    par(fig = c(0, 0.5, 0, 0.5), new = T)
    h = hist(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), breaks="Scott", plot=TRUE, main = "Outgoing/Incoming speed (m/s)",
             xlab = "Outgoing/Incoming speed (m/s) [R2]", ylab = "Density", cex.lab = 1.2,  freq = FALSE)
    text(Angle.speed, max(h$density), labels = paste("R2 =", Angle.speed), col = "blue", cex = 0.8) # User defined 'Angle.speed [R2]'
    text(quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.95, na.rm = TRUE), max(h$density)/1.7, labels = "95%", col = "green", cex = 0.8) # 95%
    text(quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE), max(h$density)/3.2, labels = "99%", col = "red", cex = 0.8) # 99%
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density) - max(h$density)/10)
    abline(v = Angle.speed, col="blue", lwd=1.5, lty=2) # User defined 'Dist.thresh [R1]'
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.95, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    #Zoom in
    par(fig = c(0.2, 0.49, 0.15, 0.49), new = T)
    h = hist(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), breaks="Scott", plot=TRUE, main = "",
             xlab = "", ylab = "", freq = FALSE, xlim = c(0, quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE)))
    if(Angle.speed < quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE)){
      text(Angle.speed, max(h$density), labels = paste("R2 =", Angle.speed), col = "blue", cex = 0.8) # User defined 'Angle.speed [R2]'
    }
    text(quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.95, na.rm = TRUE), max(h$density)/1.7, labels = "95%", col = "green", cex = 0.8) # 95%
    text(quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE), max(h$density)/3.2, labels = "99%", col = "red", cex = 0.8) # 99%
    if(Angle.speed < quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE)){
      clip(x1 = 0,
           x2 = max(h$breaks), 
           y1 = 0,
           y2 = max(h$density) - max(h$density)/10)
      abline(v = Angle.speed, col="blue", lwd=1.5, lty=2) # User defined 'Dist.thresh [R1]'
    }
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.95, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    
    #(4) Distribution of Maximum speed (R3)
    par(fig = c(0.5, 1, 0, 0.5), new = T)
    h = hist(df.sub$Maximum.speed, breaks="Scott", plot=TRUE, main = "Maximum speed (m/s)",
             xlab = "Maximum speed (m/s) [R3]", ylab = "Density", cex.lab = 1.2,  freq = FALSE)
    text(Max.speed, max(h$density), labels = paste("R3 =", Max.speed), col = "blue", cex = 0.8) # User defined 'Max.speed [R3]'
    text(quantile(df.sub$Maximum.speed, 0.95, na.rm = TRUE), max(h$density)/1.7, labels = "95%", col = "green", cex = 0.8) # 95%
    text(quantile(df.sub$Maximum.speed, 0.99, na.rm = TRUE), max(h$density)/3.2, labels = "99%", col = "red", cex = 0.8) # 99%
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density) - max(h$density)/10)
    abline(v = Max.speed, col="blue", lwd=1.5, lty=2) # User defined 'Dist.thresh [R1]'
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.95, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), 0.99, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    #Zoom in
    par(fig = c(0.7, 0.99, 0.15, 0.49), new = T)
    h = hist(c(df.sub$Outgoing.speed, df.sub$Incoming.speed), breaks="Scott", plot=TRUE, main = "",
             xlab = "", ylab = "", freq = FALSE, xlim = c(0, quantile(df.sub$Maximum.speed, 0.99, na.rm = TRUE)))
    if(Max.speed < quantile(df.sub$Maximum.speed, 0.99, na.rm = TRUE)){
      text(Max.speed, max(h$density), labels = paste("R3 =", Max.speed), col = "blue", cex = 0.8) # User defined 'Angle.speed [R2]'
    }
    text(quantile(df.sub$Maximum.speed, 0.95, na.rm = TRUE), max(h$density)/1.7, labels = "95%", col = "green", cex = 0.8) # 95%
    text(quantile(df.sub$Maximum.speed, 0.99, na.rm = TRUE), max(h$density)/3.2, labels = "99%", col = "red", cex = 0.8) # 99%
    if(Max.speed < quantile(df.sub$Maximum.speed, 0.99, na.rm = TRUE)){
      clip(x1 = 0,
           x2 = max(h$breaks), 
           y1 = 0,
           y2 = max(h$density) - max(h$density)/10)
      abline(v = Angle.speed, col="blue", lwd=1.5, lty=2) # User defined 'Dist.thresh [R1]'
    }
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/2)
    abline(v = quantile(df.sub$Maximum.speed, 0.95, na.rm = TRUE), col="green", lwd=1.5, lty=2) #95% quantile
    clip(x1 = 0,
         x2 = max(h$breaks), 
         y1 = 0,
         y2 = max(h$density)/4)
    abline(v = quantile(df.sub$Maximum.speed, 0.99, na.rm = TRUE), col="red", lwd=1.5, lty=2) #99 % quantile
    
    Thresholds <- recordPlot() #Save plot
    
    ###ggplot Results of filtering (user-defined thresholds)###
    p1 = ggplot(df.sub, aes(x = Longitude, y = Latitude,
                            text =  paste('Timestamp: ', Timestamp,
                                          '</br> Fix number: ', Fix.number,
                                          '</br> Window group: ', Window.group,
                                          '</br> Distance from median (m): ', round(Dist.from.median, 1),
                                          '</br> Angle between 3 fixes (°): ', round(Ang.vertex, 1),
                                          '</br> Outgoing speed of 3 fixes (m/s): ', round(Outgoing.speed, 2),
                                          '</br> Incoming speed of 3 fixes (m/s): ', round(Incoming.speed, 2),
                                          '</br> Maximum speed (m/s): ', round(Maximum.speed, 2),
                                          '</br> R1: ', R1,
                                          '</br> R2: ', R2,
                                          '</br> R3: ', R3)))+
      geom_path(aes(group=1), size = 0.3,  alpha = 0.5, color = "grey30")+
      geom_point(aes(color = Verdict), alpha = 0.8)+
      ggtitle("Gundog GPS filter")+
      xlab("Longitude")+ 
      ylab("Latitude")+
      scale_color_manual(name = "Outlier detection:",
                         labels = c("Not anomalous" ,"Possible outlier", "Unanimous outlier"),
                         values = c("green", "darkorange2" ,"red"),
                         na.translate = FALSE,
                         drop = FALSE)+
      coord_equal(ratio = 1) + #+ scale_x_continuous(labels=scaleFUN)+scale_y_continuous(labels=scaleFUN)
      theme_bw()+
      theme(axis.text.x = element_text(color = "black", size = 12),
            axis.text.y = element_text(color = "black", size = 12),
            axis.title.x = element_text(color = "black", size = 16),
            axis.title.y = element_text(color = "black", size = 16),
            plot.title = element_text(hjust = 0.5),
            legend.position = "none") 
    
    ###ggplot Results of filtering (Isolation forest)####
    p2 = ggplot(df.sub, aes(x = Longitude, y = Latitude,
                            text =  paste('Timestamp: ', Timestamp,
                                          '</br> Fix number: ', Fix.number,
                                          '</br> Window group: ', Window.group,
                                          '</br> Distance from median (m): ', round(Dist.from.median, 1),
                                          '</br> Angle between 3 fixes (°): ', round(Ang.vertex, 1),
                                          '</br> Outgoing speed of 3 fixes (m/s): ', round(Outgoing.speed, 2),
                                          '</br> Incoming speed of 3 fixes (m/s): ', round(Incoming.speed, 2),
                                          '</br> Maximum speed (m/s): ', round(Maximum.speed, 2),
                                          '</br> I.F_anomaly_score_R1: ', round(I.F_anomaly_score_R1, 2),
                                          '</br> I.F_anomaly_score_R2: ', round(I.F_anomaly_score_R2, 2),
                                          '</br> I.F_anomaly_score_R3: ', round(I.F_anomaly_score_R3, 2))))+
      geom_path(aes(group=1), size = 0.3,  alpha = 0.5, color = "grey30")+
      geom_point(aes(color = Verdict_IF), alpha = 0.8)+
      ggtitle("Gundog GPS filter")+
      xlab("Longitude")+ 
      ylab("Latitude")+
      scale_color_manual(name = "Outlier detection:",
                         labels = c("Not anomalous" ,"Possible outlier", "Unanimous outlier"),
                         values = c("green", "darkorange2" ,"red"),
                         na.translate = FALSE,
                         drop = FALSE)+
      coord_equal(ratio = 1) + #+ scale_x_continuous(labels=scaleFUN)+scale_y_continuous(labels=scaleFUN)
      theme_bw()+
      theme(axis.text.x = element_text(color = "black", size = 12),
            axis.text.y = element_text(color = "black", size = 12),
            axis.title.x = element_text(color = "black", size = 16),
            axis.title.y = element_text(color = "black", size = 16),
            plot.title = element_text(hjust = 0.5),
            legend.position = "none") 
    
    #Make Interactive and plot user-defined, and Isolation Forests results side-by-side
    fig <- subplot(ggplotly(p1), ggplotly(p2)) 
    #Add sub headings
    annotations = list( 
      list( 
        x = 0.2,  
        y = 1.0,  
        text = "User-defined threholds",  
        xref = "paper",  
        yref = "paper",  
        xanchor = "center",  
        yanchor = "bottom",  
        showarrow = FALSE 
      ),  
      list( 
        x = 0.8,  
        y = 1,  
        text = "Isolation forest",  
        xref = "paper",  
        yref = "paper",  
        xanchor = "center",  
        yanchor = "bottom",  
        showarrow = FALSE 
      ))
    
    fig <- fig %>%layout(annotations = annotations) 
    print(fig)
    return(list(Thresholds = Thresholds, Plot = fig, df = as.data.frame(df.sub)))
    par(mfrow = c(1,1)) #Return plotting parameters back
  }else{ return(df = as.data.frame(df.sub))
  } #If no plotting, just return the data frame
}

#END OF FUNCTION

#For example.... (assuming original GPS data set is called 'Drogon')
#df.sub = Gundog.GPS.Filter(TS = Drogon$study.local.timestamp, Longitude = Drogon$location.long, Latitude = Drogon$location.lat, Drop.out = 300, Window.length = 10, Burst.method = "median", Dist.thresh = 50, Max.speed = 0.85, Speed.step = 1, Angle.speed = 0.85, Angle.thresh = 30, plot = TRUE, I.F_conf = 0.995)

#And to recover the data frame from the returned list...
#GPS.df = df.sub[["df"]]
```

A test case on one bird was used to refine the user input threshold values until visual inspection showed good performance on reducing obvious outliers. We removed duplicate values and fixes taken with less than 4 satellites using the dupfilter from SDLfilter. 

```{r speed filter test}
# Trial with one bird
prem <- filter(data_raw, id == "Prem") %>%
  dupfilter(); beep()

speed_filt <- Gundog.GPS.Filter(
  TS = prem$DateTime,
  Longitude = prem$lon,
  Latitude = prem$lat,
  Drop.out = 600, # grouping window
  Burst.method = "none",
  Dist.thresh = 714, #m - max distance per time window
  Angle.speed = 1.07, #m/s
  Angle.thresh = 30,
  Max.speed = 1.19, #m/s
  Speed.step = 1, # number of fixes to calculate speed
  plot = FALSE # to get just dataframe output, otherwise TRUE for list including plots
); beep()

speed_filt <- speed_filt %>%
  rename(DateTime = Timestamp) %>%
  left_join(prem)

# Plot data cleaning to check effect of filtering
ggmap(map_z13)+
  geom_path(data=speed_filt, aes(Longitude, Latitude), colour = "blue")+
  geom_point(data=filter(speed_filt, Verdict_IF == "Possible outlier"),
             aes(Longitude, Latitude), colour = "orange")+
  geom_point(data=filter(speed_filt, Verdict_IF == "Unanimous outlier"), 
             aes(Longitude, Latitude), colour = "yellow")+
  geom_point(data=filter(speed_filt, Verdict_IF == "Not anomalous"), 
             aes(Longitude, Latitude), colour = "blue")
```

We applied the speed-based filtering to all bird tracks. 

```{r speed filter}
# Apply duplicate and qi filter with SDLfilter using default qi=4 🐢
data_dupfilt <- dupfilter(data_raw); beep()

# Loop through all birds 🐢🐢
speed_filt <- data.frame()

for(i in 1:length(birds)){
 subset <- filter(data_dupfilt, id == birds[i])
 
 temp <- Gundog.GPS.Filter(
  TS = subset$DateTime,
  Longitude = subset$lon,
  Latitude = subset$lat,
  Drop.out = 600, # grouping window
  Burst.method = "none",
  Dist.thresh = 714, #m - max distance per time window
  Angle.speed = 1.07, #m/s
  Angle.thresh = 30,
  Max.speed = 1.19, #m/s
  Speed.step = 1, # number of fixes to calculate speed
  plot = FALSE # to get just dataframe output, otherwise TRUE for list including plots
 ) %>%
  rename(DateTime = Timestamp) %>%
  left_join(subset)
  
 speed_filt <- rbind(speed_filt, temp)
 
 print(paste("Completed processing for", as.character(birds[i])))
 
}; beep()

# Plot filtering by bird
ggplot()+
  geom_path(data=speed_filt, aes(Longitude, Latitude), colour = "black", alpha = .6)+
  geom_point(data=filter(speed_filt, Verdict_IF == "Possible outlier"),
             aes(Longitude, Latitude), colour = "purple")+
  geom_point(data=filter(speed_filt, Verdict_IF == "Unanimous outlier"), 
             aes(Longitude, Latitude), colour = "red")+
  geom_point(data=filter(speed_filt, Verdict_IF == "Not anomalous"), 
             aes(Longitude, Latitude), colour = "black")+
  facet_wrap(~id) + 
  theme_void()

# Save to file - flagged outliers
write.csv(speed_filt, "Processed/GPSdata_flagged.csv", row.names = FALSE)

# Save to file - smaller version with minimal columns and removed outliers
speed_filt2 <- speed_filt %>%
  filter(Verdict_IF == "Not anomalous") %>%
  select(c("id", "timestamp", "study_local_timestamp", "lon", "lat", "acceleration_raw_x", "acceleration_raw_y", "acceleration_raw_z", "external_temperature", "height_above_msl", "utm_easting", "utm_northing", "utm_zone", "ground_speed", "heading"))

write.csv(speed_filt2, "Processed/GPSdata_filtered.csv", row.names = FALSE)
```

dupfilter_exact removed 2 of 1034948 locations
dupfilter_qi removed 2877 of 1034946 locations
dupfilter_time removed 1 of 1032069 locations
dupfilter_space removed 1315 of 1032068 locations

Input data: 1034948 locations
Filtered data: 1030753 locations
dupfilter removed 4195 locations (0.41% of original data)

And with the gundog.gps.filter
Input data: 1030753 locations
Filtered data: 1016434
Gundog removed 14319 locations (1.3892% of original data)

### Median smoothing
Even after speed/angle filtering, we retain some smaller-scale 'jitter'- these are challenging to remove as they lie within the bounds of realistic movement. Median resampling is a method of smoothing the track to reduce jitter. 

First a test subset. 

```{r smoothing test subset}
# Read in filtered data
data <- read_csv("Processed/GPSdata_filtered.csv") %>%
  mutate(datetime = as.POSIXct(study_local_timestamp, "%Y-%m-%d %H:%M:%S"),
         date = as.Date(datetime))

# Test subset, 1 month for Robin
robin <- filter(data, id == "Robin" & date %in% as_date(c(as_date("2023-03-01"):as_date("2023-03-30"))))

# Median smoothing
robin_smooth <- atl_median_smooth(data = robin, x = "lat", y = "lon",
                                  time = "DateTime", moving_window = 3)

# And errors to plot
error <- filter(data, id == "Robin" & date %in% as_date(c(as_date("2023-03-01"):as_date("2023-03-30")))) %>%
  filter(!Verdict_IF == "Not anomalous") 
  
# Plot smoothed track over original points
ggplot()+
  geom_point(data = robin, aes(lon, lat))+
  geom_point(data = error, aes(lon, lat), colour = "red")+
  geom_path(data = robin_smooth, aes(lon, lat, colour = DateTime))+
  scale_colour_viridis_c()+
  theme_void()

# And against satellite map
ggmap(map_z15)+
  geom_point(data = robin, aes(lon, lat), colour = "white", inherit.aes = FALSE)+
  geom_point(data = error, aes(lon, lat), colour = "red")+
  geom_path(data = robin_smooth, aes(lon, lat, colour = DateTime), inherit.aes = FALSE)+
  scale_colour_viridis_c()+
  theme_void()
```

Happy with that - smoothing window of three doesn't lose track verismilitude and reduced jitter. Running with all birds.

```{r smoothing}
# Read in unfiltered data and remove outliers
data <- read_csv("Processed/GPSdata_filtered.csv") %>%
  mutate(datetime = as.POSIXct(study_local_timestamp, "%Y-%m-%d %H:%M:%S"),
         date = as.Date(datetime))

# Median smoothing by bird
smooth <- data.frame()

for(i in 1:length(birds)){
 subset <- filter(data, id == birds[i])
 
 temp <- atl_median_smooth(data = subset, x = "lat", y = "lon",
                                  time = "datetime", moving_window = 3)
 smooth <- rbind(smooth, temp)
 
}; beep()

# Plot smoothed tracks by bird
ggplot()+
  geom_path(data = smooth, aes(lon, lat))+
  scale_colour_viridis_d()+
  theme_void()+
  facet_wrap(~id)

# Save to file
write.csv(smooth, "Processed/GPSdata_smoothed.csv", row.names = FALSE)
```

### Manual removal
There are a handful of displacement outliers remaining that I'm going to remove manually based on visual identification. I will remove points where the lat/lon exceed simultaneous thresholds. 

```{r manual outliers}
# Read in smoothed data
smooth <- read_csv("Processed/GPSdata_smoothed.csv")

# Map for visual checks (sub in bird name each time)
ggmap(map_z14)+
  geom_path(data = subset(smooth, id == "Valentine"), aes(lon, lat), colour = "yellow", inherit.aes = FALSE)+
  theme_minimal()

# Marmalade
marma <- subset(smooth, id == "Marmalade") %>%
  filter(lon < 144.445)

# Nutmeg
nutmeg <- subset(smooth, id == "Nutmeg") %>%
  filter(lat < -37.8915)

# Briar
briar <- subset(smooth, id == "Briar") %>%
  filter(lon < 144.448) %>%
  filter(lat < -37.892 | lon < 144.42) %>%
  filter(lon > 144.439 | lat < -37.894)

# Koda
koda <- subset(smooth, id == "Koda") %>%
  filter(lat < -37.892 & lon < 144.444) %>%
  filter(lat > -37.91)

# Prem
prem <- subset(smooth, id == "Prem") %>%
  filter(lat < -37.893) %>%
  filter(lon > 144.430 | lat < -37.897) %>%
  filter(lon < 144.442 | lat > -37.901)

# Robin
robin <- subset(smooth, id == "Robin") %>%
  filter(lat < -37.892) %>%
  filter(lon < 144.448 | lat < -37.895) %>%
  filter(lon > 144.438 | lat < -37.894) %>%
  filter(lon > 144.429 | lat < -37.901) %>%
  filter(lon > 144.425)

# Replace the 4 birds above in the smooth data
smooth2 <- smooth %>%
  filter(!id %in% c("Marmalade", "Nutmeg", "Briar", "Koda", "Prem", "Robin")) %>%
  rbind(marma, nutmeg, briar, koda, prem, robin)

# Plot cleaned tracks by bird
ggplot()+
  geom_path(data = smooth2, aes(lon, lat))+
  scale_colour_viridis_d()+
  theme_void()+
  facet_wrap(~id)

# Write out cleaned data
write.csv(smooth2, "Processed/GPSdata_clean.csv", row.names = FALSE)
```

Input smoothed data: 1016434
After manual cleaning: 1015862
Removed: 572 (0.056% original data)

## Time over fence

Originally, we were going to consider survival as a binary (yes/no to 90 days and 1 year - 360 days to be precise, which is the minimum common denominator of tracking time from deployment to retrieval of GPS units) and persistence (duration survived, integer data). However, the risk of mortality is not actually equal for all birds across time, since (due to variations in primary moult) some were able to leave the fenced  area sooner than others, which has a much higher risk factor. To account for this, I calculate the time spent over the fence per bird, as a proxy for increased predation risk. Time spent over fence is calculated as duration track sections outside of the polygon. 

First need to define the fences. 

```{r MR fences}
# Define fence polygons
zone1 <- rbind( # clockwise
  c(-37.897319, 144.429048), # S end of NW diagonal
  c(-37.894066, 144.432334), # N end of NW diagonal, i.e. NW corner
  c(-37.894749, 144.438305), # bend at main gate 
  c(-37.894693, 144.438324), # main gate
  c(-37.894803, 144.439214), # bend before N Z1/2 gate
  c(-37.894718, 144.439337), # N Z1/2 gate, i.e. NE corner
  c(-37.898143, 144.440381), # S Z1/2 gate
  c(-37.898280, 144.440662), # Z1/btrw pen NW corner
  c(-37.900692, 144.440368), # Z1/btrw SW corner
  c(-37.902610, 144.443002), # Z1/btrw SE corner
  c(-37.909538, 144.439648), # Z1 SE corner
  c(-37.908999, 144.434965), # Z1 southern boundary bend 1
  c(-37.907537, 144.433359), # Z1 southern boundary bend 2
  c(-37.905958, 144.430140), # Z1 southern boundary bend 3
  c(-37.905486, 144.429479), # Z1 southern boundary bend 4
  c(-37.904649, 144.427566), # Z1 SW corner
  c(-37.897319, 144.429048)  # S end of NW diagonal
) %>%  vect(type = "polygons", crs = "EPSG:4326") %>%
  t() %>%
  st_as_sf() %>%
  cbind(data.frame(zone = "zone1"))

zone2 <- rbind( #clockwise
  c(-37.894718, 144.439337), # N Z1/2 gate, i.e. SW corner
  c(-37.892433, 144.440236), # N boundary internal aviary/Z2 i.e. NW corner
  c(-37.892803, 144.443374), # Z2 northern boundary bend 1
  c(-37.893634, 144.444076), # Z2 northern boundary bend 2
  c(-37.894585, 144.447739), # N end of Z2/Z3 boundary, i.e. NE corner
  c(-37.896515, 144.446789), # Z2/Z3 boundary bend 1
  c(-37.896726, 144.446099), # Z2/Z3 boundary bend 2
  c(-37.897964, 144.445706), # Z2/Z3 boundary bend 3
  c(-37.899960, 144.444517), # S end of Z2/Z3 boundary, i.e. SE corner
  c(-37.899908, 144.444288), # Z2/btrw NE corner
  c(-37.898280, 144.440662), # Z1/btrw pen NW corner
  c(-37.898143, 144.440381), # S Z1/2 gate
  c(-37.894718, 144.439337)  # N Z1/2 gate, i.e. SW corner
) %>%  vect(type = "polygons", crs = "EPSG:4326") %>%
  t() %>%
  st_as_sf() %>%
  cbind(data.frame(zone = "zone2"))

aviary <- rbind( # anticlockwise
  c(-37.894693, 144.438324), # inner main gate W end, i.e. SW corner
  c(-37.894803, 144.439214), # bend before N Z1/2 gate
  c(-37.894718, 144.439337), # N Z1/2 gate, i.e. SE corner
  c(-37.892433, 144.440236), # N boundary internal aviary/Z2, i.e. NE corner
  c(-37.892254, 144.438762), # outer main gate W end, i.e. NW corner
  c(-37.894693, 144.438324)  # inner main gate W end, i.e. SW corner
) %>%  vect(type = "polygons", crs = "EPSG:4326") %>%
  t() %>%
  st_as_sf() %>%
  cbind(data.frame(zone = "aviary"))

btrw <- rbind(
  c(-37.898280, 144.440662), # Z1/btrw pen NW corner
  c(-37.899908, 144.444288), # Z2/btrw NE corner
  c(-37.902610, 144.443002), # Z1/btrw SE corner
  c(-37.900692, 144.440368), # Z1/btrw SW corner
  c(-37.898280, 144.440662)  # Z1/btrw pen NW corner
) %>%  vect(type = "polygons", crs = "EPSG:4326") %>%
  t() %>%
  st_as_sf() %>%
  cbind(data.frame(zone = "btrw"))

zone3 <- rbind( # anticlockwise
  c(-37.894585, 144.447739), # N end of Z2/Z3 boundary
  c(-37.896515, 144.446789), # Z2/Z3 boundary bend 1
  c(-37.896726, 144.446099), # Z2/Z3 boundary bend 2
  c(-37.897964, 144.445706), # Z2/Z3 boundary bend 3
  c(-37.899960, 144.444517), # S end of Z2/Z3 boundary, i.e. SW corner
  c(-37.902315, 144.459277), # SE corner
  c(-37.896942, 144.460285), # Z3 E boundary bend 1
  c(-37.896869, 144.460432), # Z3 E boundary bend 2
  c(-37.897357, 144.464663), # Z3 E boundary bend 3, E most point
  c(-37.883947, 144.457228), # Z3 E boundary bend 4
  c(-37.882432, 144.455531), # NE corner
  c(-37.881765, 144.450057), # NW corner
  c(-37.894585, 144.447739) # N end of Z2/Z3 boundary
) %>%  vect(type = "polygons", crs = "EPSG:4326") %>%
  t() %>%
  st_as_sf() %>%
  cbind(data.frame(zone = "zone3"))

# Combine polygons
mtr <- rbind(zone1, zone2, zone3, aviary, btrw)

# Plot on map
ggmap(map_z14) +
  geom_spatvector(data=mtr, inherit.aes = FALSE, alpha = .4,
                  aes(fill = zone))+
  scale_fill_viridis_d()+
  theme_void()

# Save fence shapefiles
st_write(mtr, "mtr_fences/mtr_fences.shp", append = FALSE)

```

Then bring in the cleaned data and allocate points as in/out of fences. 

```{r in/out fence}
# Read in clean data 🐢
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(datetime = as.POSIXct(datetime)); beep()

# Convert points to spatial
data_sp <- vect(data, geom = c("lon", "lat"), crs = "EPSG:4326")

# Read in fence
fence <- vect("mtr_fences/mtr_fences.shp")

# Since GPS error is up to 120m going to make the 'outside' fence buffer
fence_buff <- fence %>%
  st_as_sf() %>%
  st_union() %>%
  vect() %>%
  project("EPSG:32755") %>%
  buffer(width = 120) %>%
  project("EPSG:4326") 

# Find points outside fence
out_sp <- mask(data_sp, fence_buff, inverse = TRUE) %>%
  # Filter out errors on the southern fenceline and at the roost in northern Z2
  erase(ext(144.425, 144.435, -37.91, -37.902)) %>%
  erase(ext(144.44, 144.45, -37.894, -37.890)) %>%
  # Filter out single erroneous Aurora point
  filter(!id == "Aurora")

# Plot on map
ggmap(map_z14) +
  geom_spatvector(data=fence_buff, inherit.aes = FALSE, alpha = .4)+
  scale_fill_viridis_d()+
  geom_spatvector(data=out_sp, aes(colour = id), inherit.aes = FALSE)+
  theme_void()

# Convert back to data.frame
out <- out_sp %>%
  st_as_sf() %>%
  st_drop_geometry()

# Summarise
out_sum <- out %>% group_by(id) %>%
  summarise(np = length(id),
            first_out = as_date(min(date)),
            last_out = as_date(max(date)),
            days = length(unique(date))) %>%
  # Add row for Sage who died out but so few points it was filtered out
  rbind(data.frame(
    id = "Sage",
    np = NA,
    first_out = as_date("2023-06-27"),
    last_out = as_date("2023-06-27"),
    days = 1 
  )) %>%
  # Add a row for Clover who died right near the front gate
    rbind(data.frame(
    id = "Clover",
    np = NA,
    first_out = as_date("2023-04-29"),
    last_out = as_date("2023-04-29"),
    days = 1 
  )) %>%
  # Lastly, a row for Nutmeg who died at Ford, but no tracking data to there
      rbind(data.frame(
    id = "Nutmeg",
    np = NA,
    first_out = as_date("2024-01-07"),
    last_out = as_date("2024-01-07"),
    days = 1 
  ))
```

They actually all spend very few days over the fence except Marmalade. Many of the over fence mortalities die on their first or second day out. Also no points for Sage over the fence - mortality must have been immediately after going over.
How many days post-release are they going over?

```{r over fence days}
# Read in survival data
survival <- read.csv("Input/survival.csv") %>%
  clean_names() %>%
  rename(id = identity) %>%
  filter(id %in% birds) %>%
  mutate(start_date = as_date(start_date, format = "%d/%m/%Y"),
         end_date = as_date(end_date, format = "%d/%m/%Y")) %>%
  select(c(id, start_date, end_date, to_360))

# Join over fence and survival
out_sum2 <- left_join(out_sum, survival) %>%
  mutate(days_to_out = first_out - start_date) %>%
  mutate(days_out_to_dead = ifelse(to_360 == "TRUE", NA, end_date - first_out))
```


## Distance

We calculated two distance metrics: 
  1) distance from release location
  2) total distance moved daily
  
### Distance from release location
This gives an indication of the bird's propensity to explore the release site and exploit new resources. 

```{r distance from release}
# Read in clean data
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(datetime = as.POSIXct(datetime)); beep()

# Specify the coords of each GPS fix
coords <- as.matrix(cbind(data$utm_easting, data$utm_northing))

# Set coords for the release location 
release  <- matrix(c(274423.45, 5801912.85), ncol=2) #(-37.902374, 144.434323)

# Calculate distance between the release location and all fixes
dist <- mutate(data, dist_release = distance(coords, as.matrix(release), lonlat=FALSE))

# Calculate daily summary statistics
dist_day <- dist %>% group_by(date, id) %>% 
  dplyr::summarise(mean = mean(dist_release),
                   max = max(dist_release),
                   min = min(dist_release)) %>%
  mutate(date = as_date(date))

# Plot average distance from release site per day, smoothed & coloured by individual
ggplot(dist_day)+
  geom_smooth(aes(date, mean, group = id, color = id))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Mean daily distance from release location (m)")+
  scale_colour_viridis_d()

# Add release group information
group <- data.frame(
  id = c("Aurora", "Robin", "Briar", "Nutmeg", 
         "Iona", "Sage", "Koda",
         "Prem", "Rove", "Brook", "Clover", "Wobbles", "Valentine", "Marmalade"),
  release = c(rep("October",4), rep("December",3), rep("January",7)))

dist_day <- left_join(dist_day, group)        

# Plot average distance from release site per day, smoothed & coloured by release group
ggplot(dist_day)+
  geom_smooth(aes(date, mean, group = id, color = release))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Mean daily distance from release location (m)")+
  scale_color_manual(values = c("#881C00FF","#1BB6AFFF","#172869FF"))+
  theme(axis.text.x = element_text(angle = 90))

# And summary stats for whole period
dist_summary <- dist_day %>%
  group_by(id) %>%
  summarise(mean_daily_dist = mean(mean))

# Save results
write.csv(dist_day, "Output/distance_release.csv", row.names = FALSE)
write.csv(dist_summary, "Output/distance_release_mean.csv", row.names = FALSE)
```

### Distance moved per day
Total distance moved daily gives an indication of the activity levels of the bird.

```{r distance moved daily}
# Read in clean data
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(datetime = as.POSIXct(datetime)); beep()

# Convert to track (AMT format)
track <- make_track(data, .x = utm_easting,.y= utm_northing,
                    .t= datetime, id = id) %>%
  nest(data = -"id")

# Loop through distance per day per bird 🐢
dates <- unique(data$date)
dist <- data.frame()

for (i in 1:length(birds)){ 
  for (j in 1:length(dates)){
    
    t <-  track$data[[i]] %>%
      mutate(date = date(t_)) %>%
      filter(date==dates[j])
    
    if (nrow(t)<2){
      next
    }
    
    out <- data.frame(dist= cum_dist(t)/1000,
                      bird = track$id[[i]],
                      date = as_date(dates[j]))
    
    print(out)
    
    dist <- rbind.data.frame(dist, out)
    
  }}; beep()

# Plot distance moved per day, smoothed and coloured by individual
ggplot(dist)+
  geom_smooth(aes(date, dist, group = bird, color=bird))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Total daily distance moved (km)")+
  scale_color_viridis_d()

# Add release group information
group <- data.frame(
  bird = c("Aurora", "Robin", "Briar", "Nutmeg",
         "Iona", "Sage", "Koda",
         "Prem", "Rove", "Brook", "Clover", "Wobbles", "Valentine", "Marmalade"),
  release = c(rep("October",4), rep("December",3), rep("January",7)))

dist <- left_join(dist, group)  

# Plot distance moved per day, smoothed & coloured by release group
ggplot(dist)+
  geom_smooth(aes(date, dist, group = bird, color = release))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Total daily distance moved (km)")+
  scale_color_manual(values = c("#881C00FF","#1BB6AFFF","#172869FF"))+
  theme(axis.text.x = element_text(angle = 90))

# and as an average over the study period
dist_summary <- dist %>% 
  group_by(bird) %>%
  summarise(mean_daily_dist = mean(dist))

# save results
write.csv(dist, "Output/distance_daily.csv", row.names = FALSE)
write.csv(dist_summary, "Output/distance_daily_mean.csv", row.names = FALSE)
```

## Home range

Core 50% KUD and full 90% KUD home range using kernel utilisation density. 

```{r home range}
# Read in clean data
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(datetime = as.POSIXct(datetime)); beep()

# Convert GPS fixes to spatial points data frame
locs <- SpatialPointsDataFrame(coordinates(
  cbind(data$utm_easting, data$utm_northing)),
  data = dplyr::select(data, id))

# calculate home range 
## 90% kud
hr_90 <-  kernelUD(locs, h="href", grid=200) %>% 
  getverticeshr(percent = 90); beep()

## 50% kud
hr_50 <-  kernelUD(locs, h="href", grid=200) %>% 
  getverticeshr(percent = 50); beep()

# transform to latlon
## 90% kud
proj4string(hr_90) <- CRS("EPSG:32755") #set crs
hr_90ll <- spTransform(hr_90, CRS("EPSG:4326")) %>% #transform to lat lon
  st_as_sf()

## 50% kud
proj4string(hr_50) <- CRS("EPSG:32755") #set crs
hr_50ll <- spTransform(hr_50, CRS("EPSG:4326")) %>% #transform to lat lon
  st_as_sf()

# plot map
## 90% kud
ggmap(map_z15)+
  geom_sf(data=hr_90ll, aes(fill=id), alpha = .7, inherit.aes = FALSE) +
  scale_fill_viridis_d()+
  theme_void()

## 50% kud
ggmap(map_z15)+
  geom_sf(data=hr_50ll, aes(fill=id), alpha = .7, inherit.aes = FALSE) +
  scale_fill_viridis_d()+
  theme_void()

# save results
hr <- data.frame(bird = hr_90@data$id,
                 kud90 = hr_90@data$area,
                 kud50 = hr_50@data$area)

write.csv(hr, "Output/hr_area.csv", row.names = FALSE)
```

And daily 90% home range

```{r home range daily}
# Read in clean data
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(datetime = as.POSIXct(datetime)); beep()

# convert GPS fixes to spatial points data frame
locs <- SpatialPointsDataFrame(coordinates(
  cbind(data$utm_easting, data$utm_northing)),
  data = data)

# loop through birds and days for 90% KUD 🐢🐢🐢 expect 5-8 hours
dates <- unique(locs$date)

hr_daily <- data.frame()

for (i in 1:length(birds)){
  for (j in 1:length(dates)){
    points <- subset(locs, id == birds[i] & date == dates[j],
                     select = id)
    if (length(points)<5){
      next
    }
    kud <- kernelUD(points[,1], h="href", grid=1000, extent = 5) %>% 
      getverticeshr(percent = 90) %>%
      st_as_sf() %>%
      mutate(date = as_date(dates[j])) %>%
      st_drop_geometry()

    print(head(kud, n= 1L))
    
    hr_daily <- rbind.data.frame(hr_daily, kud)
  }}; beep()

# Save to file
write.csv(hr_daily, "Output/hr_daily_area.csv", row.names = FALSE)

# Plot daily home ranges, smoothed and coloured by individual
ggplot(hr_daily)+
  geom_smooth(aes(date, area, group = id, color=id), method = "lm")+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Daily 90% KDE home range (ha)")+
  scale_color_viridis_d()+
  theme(axis.text.x = element_text(angle = 90))

# And individual box plots
ggplot(hr_daily)+
  geom_boxplot(aes(id, area, color = id))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  labs(x= element_blank(), y = "Daily 90% KDE home range (ha)")+
  theme(axis.text.x = element_text(angle = 50))+
  scale_color_viridis_d()+
  theme(legend.position="none")

```

## Sociality

Note to self: version 1.0 of wildlifeDI moved from ltraj objects to move2 object. Need to update code- next chunk doesn't work right now. 

```{r sociality}
# Read in clean data 
data <- read.csv("Processed/GPSdata_filtered.csv") %>%
  mutate(DateTime = as.POSIXct(study_local_timestamp, 
                                 "%Y-%m-%d %H:%M:%S",tz = "Australia/Melbourne"),
         date = as.Date(DateTime, tz = "Australia/Melbourne"))

# Set up data in move2 format for wildlifeDI
data_move <- mt_as_move2(data, 
                         time_column = "DateTime",
                         track_id_column = "id",
                         coords =  c("utm_easting", "utm_northing")) %>%
  sf::st_set_crs("EPSG:32755")

# All bird combinations
list <-combinations(n = 15, r = 2, v = 1:15, repeats.allowed = FALSE)
list1 <- list[,1]
list2 <- list[,2]

# Calculate interactions between all birds 🐢🐢
interact <- data.frame()

for(i in 1:length(list1)) {
  
    prox <- tryCatch({
      data.frame(
        prox = Prox(data_move[list1[i]], data_move[list2[i]], tc=120, dc=50),
        bird1 = attr(data_move[[list1[i]]], "id"),
        bird2 = attr(data_move[[list2[i]]], "id"))
    }, error = function(e) data.frame(prox = NA, bird1 = NA, bird2 = NA))
    
    cr <- tryCatch({
      data.frame(
        cr = Cr(data_move[list1[i]], data_move[list2[i]], tc=120),
        bird1 = attr(data_move[[list1[i]]], "id"),
        bird2 = attr(data_move[[list2[i]]], "id"))
    }, error = function(e) data.frame(cr = NA, bird1 = NA, bird2 = NA))
  
    
    cs <- tryCatch({
      data.frame(
        cs = Cs(data_move[list1[i]], data_move[list2[i]], tc=120),
        bird1 = attr(data_move[[list1[i]]], "id"),
        bird2 = attr(data_move[[list2[i]]], "id"))
    }, error = function(e) data.frame(cs = NA, bird1 = NA, bird2 = NA))
  
      
    iab <- tryCatch({
      data.frame(
        iab = IAB(data_move[list1[i]], data_move[list2[i]], tc=120, dc=50),
        bird1 = attr(data_move[[list1[i]]], "id"),
        bird2 = attr(data_move[[list2[i]]], "id"))
    }, error = function(e) data.frame(iab = NA, bird1 = NA, bird2 = NA))
  
      
    di <- tryCatch({
      data.frame(
        di = DI(data_move[list1[i]], data_move[list2[i]], tc=120),
        bird1 = attr(data_move[[list1[i]]], "id"),
        bird2 = attr(data_move[[list2[i]]], "id"))
    }, error = function(e) data.frame(di = NA, bird1 = NA, bird2 = NA))
  
  
    out <- left_join(prox, cr, by = c("bird1", "bird2")) %>%
      left_join(cs, by = c("bird1", "bird2")) %>%
      left_join(iab, by = c("bird1", "bird2")) %>%
      left_join(di, by = c("bird1", "bird2"))
  
  print(paste("Finished",i))
  
  interact <- rbind(interact, out)
}

# save results
write.csv(interact, "global_interactions.csv", row.names = FALSE)

```

# Analysis

We want to know if the pre-release assays (latency to reach food, vigilance and handling reponse) can be used to predict post-release measures of performance, and thereby screen for suitable release candidates in future translocations. 

## Predictors

### Latency
Latency to reach food is how long it took each bird, in seconds, to reach a plate of food provided as part of their normal husbandry, measured from when the plate was placed on the ground until the bird took the first mouthful. 

```{r latency to reach food}
# Read in and clean up latency assay data
latency <- read_excel("Input/bsc assay data.xlsx", sheet =3) %>%
  clean_names() %>%
  # Select study birds
  filter(bird %in% birds2) %>%
  # Filter to assay dates
  filter(date %in% c("2022-10-12", "2022-10-11")) %>%
  select(c("date", "bird", "reach", "latency_s")) %>%
  # Convert yes/no to 1/0
  mutate(reach = ifelse(reach == "Yes", 1, 0)) %>%
  rename(latency = latency_s) %>%
  # Add column for plotting 'did not reach'
  mutate(label = ifelse(is.na(latency), "DNR", ""))

# Plot latency by bird
ggplot(latency) + 
  geom_bar(aes(bird, latency, fill = as.character(date)), position = "dodge", stat = "identity")+
  geom_text(aes(y=600, x= bird, label = label), angle = 90, vjust = -0.5)+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  labs(x= element_blank(), y = "Latency to reach food (seconds)")+
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  scale_fill_manual(values = c("#881C00FF","#1BB6AFFF"), name = "Assay date")+
  theme_minimal()
```

### Handling response

Handling response is an ordinal variable assigned during processing, on a scale of 1:3:

1) Calm/still/log-pose
2) Moderate/slight kicking or struggling/soft calling or beak clacking
3) Extreme/upset/shrieking/growling

```{r handling}
# Read in handling data
handling <- read_excel("Input/bsc assay data.xlsx", sheet = 7) %>%
  clean_names() %>%
  rename(bird = identity) %>%
  # Filter to study birds
  filter(bird %in% birds2) %>%
  # Filter to assay dates
  filter(date %in% c("2022-10-20"))

# Plot
ggplot(handling)+
  geom_bar(aes(score))+
  theme_minimal()
```

Interestingly, when there are repeat measures (post-release) the handling response stays the same; however, for now we are just interested in whether the pre-release assay values predict other post-release behaviours. 

### Vigilance

Vigilance is the proportion of time spent in vigilant behaviours (considered mutually exclusive from 'relaxed' behaviours). 

*Vigilant behaviours* were defined as:

* Alertness: standing but not relaxed, often startled into this pose from a more relaxed posture. May have a stiffer pose, holding neck extended or forward. May have flattened the feathers against the body. 
* Alert walk: walking with the neck stiffly extended parallel wit hthe ground. Feathers either fluffed up (held erect) or slimmed against body, as opposed to held loosely. 
* Head-bobbing: a rapid up/down movement of the head; used to communicate unease.
* Head-tilting: craning the head to the side to look above and around; possibly to check for danger (can be done in tandem with head-bobbing).
* Tail-wagging: a rapid side/side shake of the tail; used to communicate unease.
* Defensive display: holding the wings outstretched; often wings are rapidly extended which gives the impression of a black and white 'flash' of plumage.

*Relaxed behaviours* were defined as:

* Preening: actively cleaning self (i.e., running the beak along the feathers, rearranging plumage), or scratching (e.g. the face with the toes), or stretching (e.g. extending the leg and wing).
* Fluffing: a rapid erection and flattening of the feathers, often accompanied by a body and head shake. Often preceded by preening. 
* Standing: relaxed pose (in the absence of vigilant behaviours as defined above), feathers loose or lightly fluffed (as opposed to erect or flat against the body).
* Walking: stepping forward (in the absence of vigilant behaviours as defined above).
* sitting: either on tibiotarsus or fully on the ground; includes sleeping. 
* Eating: engaged in looking at, carrying, pecking, picking up, swallowing or breaking up food (or non food items that are being tested e.g. leaves). Can rapidly (second by second) change between eating and alertness. 

Read in the video assay data and calculate the time spent vigilant as a proportion of whole time. Some videos have sections where the bird is out of frame (noted in ethogram as 'Z'), so the proportion vigilant is the full minute minus the period of Z. 

```{r vigilance}
# Read in and tidy up video assays 
video <- read_excel("Input/bsc assay data.xlsx", sheet =4) %>%
  clean_names() %>%
  # Drop birds not included in study
  filter(!bird %in% c("Daisy", "Star", "Wobbles")) %>%
  # Drop metadata columns
  select(-c("pen", "clip", "time", "note")) %>%
  # Drop lines without records 
  na.omit() %>%
  # Convert from wide to long
  pivot_longer(3:62, names_to = "second", values_to = "behaviour") %>%
  # Convert seconds into numeric
  mutate(second = as.numeric(str_remove(second, "x"))) %>%
  # Assign vigilance to 1 and not to 0
  mutate(vigilant = ifelse(behaviour %in% c("A","HT","HB","HBT","N","TW","D"), 1, 0))

# Calculate time spent vigilant
vigilance <- video %>%
  group_by(bird) %>%
  summarise(z = sum(grepl("Z", behaviour)),
            L = length(behaviour) - z,
            v = sum(vigilant),
            pv = v/L)

# Plot proportion of time spent vigilant by bird
ggplot(vigilance) + 
  geom_bar(aes(bird, pv), position = "dodge", stat = "identity")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  labs(x= element_blank(), y = "Proportion of time spent vigilant")+
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  theme_minimal()
```

## Responses

Here we test the response variables (post-release performance) against the predictor variables (pre-release assays).

### Survival and persistence

Read in survival and persistence data. I set the maximum persistence duration to 360 days, because this is the maximum time tracked across all groups (time elapsed between release and removal of GPS devices).

```{r survival}
# Read in survival data
survival <- read.csv("Input/survival.csv") %>%
  clean_names() %>%
  rename(bird = identity) %>%
  filter(bird %in% birds2) %>%
  mutate(start_date = as_date(start_date, format = "%d/%m/%Y"),
         end_date = as_date(end_date, format = "%d/%m/%Y"),
         survived = ifelse(persist<360, 0, 1),
         persist = ifelse(persist>360, 360, persist))
```

#### m1a: latency ~ survival
*Does latency to reach food predict survival?*

```{r m1}
# Combine survival and latency data
m1_data <- survival %>%
  select(c("bird", "persist", "survived")) %>%
  left_join(latency)
```

There are two levels of response for latency to reach food: 

1) Did the bird reach the food (yes/no)
2) How long did it take the bird to reach the food?

First we look at the distribution of responses of whether they reached the food with survival. 

```{r reach food}
mosaicplot(table(m1_data$survived, m1_data$reach))
```

The birds didn't reach the food on one third of occasions, but all birds reached the food at least once (out of two trials). From here on we'll just use the time to reach the food.

We use a binomial family glm because the response variable (survival) is binary. I drop the NA reach values. 

```{r m1a}
m1_data <- na.omit(m1_data)

# m1a: Does latency to reach food predict survival?
ggplot(m1_data)+
  geom_boxplot(aes(x= survived, y=log(latency), group=survived))+
  theme_minimal()

m1a <- glmmTMB(survived ~ latency,
           data = m1_data,
           family = binomial)
summary(m1a)
```

There is no evidence latency predicts survival. 

Ideally I should have used date as a random factor (because some two-thirds of the birds have two latency scores), but the model wasn't able to handle this and gave me an error that "negative log-likelihood is NaN at starting parameter values". 

#### m1b: latency ~ persistence
*Does latency to reach food predict persistence?*

First, we check the distribution of the response variable (persistence) to select the appropriate glm model family. 

```{r persist}
hist(m1_data$persist)
```

The persistence values are duration (e.g. zero or higher) and have a right-skewed distribution; therefore a negative binomial regression model should be suitable.

```{r m1b}
# m1b: Does latency to reach food predict persistence?
ggplot(m1_data, aes(persist, log(latency)))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme_minimal()

m1b <- glmmTMB(persist ~ latency,
           data = m1_data,
           family = nbinom2(link = "log"))
summary(m1b)
```

No evidence that latency predicts persistence. 

#### m2a: handling response ~ survival
*Does handling response predict survival?*

```{r m2a}
# Handling and survival data
m2_data <- survival %>%
  select(c("bird", "persist", "survived")) %>%
  left_join(handling) %>%
  mutate(score = as_factor(score))

# Does handling response predict survival?
mosaicplot(table(m2_data$survived, m2_data$score))

chisq.test(table(m2_data$survived, m2_data$score))
```

While there is no evidence for a prediction here (on the chi-squared test, keeping in mind low sample size) there are some interesting observations we can make: 1) Both birds with a handling score of 3 survived. 2) No birds with a handling score of 2 survived. Perhaps an intermediate strategy is suboptimal i.e., it's better to either commit to full camouflage or full flight, not the middle ground. Perhaps the middle ground is evidence of habituation, since both scores 1 and 3 can be considered a predator-response behaviour (flight or camouflage). 

#### m2b: handling response ~ persistence
*Does handling response predict persistence?*

Since handling score is categorical, we'll use ANOVA and chi-squared to compare frequencies. 

```{r m2a}
# Does handling response predict persistence?
ggplot(m2_data)+
  geom_boxplot(aes(score, persist, group = score))
  theme_minimal()

m2b <- aov(persist ~ score,
           data = m2_data)
summary(m2b)
```

Interestingly, the persistence time for score 2 individuals is pretty long (195 & 308 days) - which contradicts my earlier thoughts about score 2 being maladaptive. Instead, it looks like there are a range of score 1 individual persistence values.

Overall, there is no evidence that handling score predicts persistence. 

#### m3a: vigilance ~ survival
*Does vigilance predict survival?*

We use a binomial family glm because the response variable (survival) is binary.

```{r m3a}
# Join vigilance and survival data
m3_data <- survival %>%
  mutate(survived = ifelse(persist<360, 0, 1),
         persist = ifelse(persist>360, 360, persist)) %>%
  select(c("bird", "persist", "survived")) %>%
  left_join(vigilance)

# m3a: Does vigilance predict survival?
ggplot(m3_data)+
  geom_boxplot(aes(survived, pv, group = survived))+
  labs(y = "Proportion of time spent vigilant")+
  theme_minimal()

m3a <- glmmTMB(survived ~ pv,
           data = m3_data,
           family = binomial)
summary(m3a)
```

#### m3b: vigilance ~ persistence
*Does vigilance predict persistence?*

As before, we use a negative binomial regression model because of the right-skewed persistence values. 

```{r m3b}
# m3b: Does vigilance predict persistence?
ggplot(m3_data, aes(persist, pv))+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(y = "Proportion of time spent vigilant")+
  theme_minimal()

m3b <- glmmTMB(persist ~ pv,
           data = m3_data,
           family = nbinom2(link = "log"))
summary(m3b)
```

Although the most vigilant bird (Iona) survived the longest, there is no evidence that vigilance (in assay setting), predicts persistence. 

That's all of the predictors against survival/persistence. Looks like none of the metrics predicted survival. But maybe we'll be able to predict sub-lethal effects or behaviours that might influence future survival and reproduction.

### Weight

We did follow-up health checks at approximately 1, 3, 6 and 12 months post-release. However, not all birds were sampled at all times. 

```{r weight}
# Read in weights from health check data
health <- read.csv("Input/healthchecks.csv") %>%
  clean_names() %>%
  rename(bird = identity) %>%
  # Filter to study birds
  filter(bird %in% birds2) %>%
  # Drop lines of Prem post-recapture
  filter(!row_number() %in% c(38,39)) %>%
# Date as date
  mutate(date = as_date(date, format = "%d/%m/%Y"))

# Plot weights over time
ggplot(health, aes(date, weight, colour = bird))+
  geom_point()+
  geom_line()
  
```

We only have post-release data for half (n=7) of the birds. I don't think this is enough to do anything with. 

### Distance moved

There are two post-release distance metrics:

1) Distance moved per day (an energetics response)
2) Distance moved from release site (a boldness or exploitation metric)

```{r distance}
# Read in distance data
dist_daily <- read.csv("Output/distance_daily.csv")
dist_release <- read.csv("Output/distance_release.csv")
```

#### m4a: latency ~ distance
*Does latency to reach food predict daily distance moved?*

## Benched stuff for coming back to later

### HMM


```{r HMM test subset}
# Read in cleaned data prepare moveHMM object
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(DateTime = as.POSIXct(DateTime, 
                                 "%Y-%m-%d %H:%M:%S",tz = "Australia/Melbourne"),
         date = as.Date(DateTime, tz = "Australia/Melbourne")) %>%
  prepData(type = "UTM", coordNames = c("utm_easting", "utm_northing"))

# Test subset, 1 month for Robin
robin <- filter(data, id == "Robin" & date %in% as_date(c(as_date("2023-03-01"):as_date("2023-03-30")))) %>%
  mutate(vedba = ((sqrt(acceleration_raw_x^2 + acceleration_raw_y^2 + acceleration_raw_x^2))/1000))

# 3 state model
## Initial parameters for gamma and von Mises distributions
### Step lengths
mu0 <- c(0,5, 20, 100) # step mean 
sigma0 <- c(1, 5, 20) # step SD 
stepPar0 <- c(mu0, sigma0) 
#### Angle concentrations
angleMean0 <- c(pi, (pi/2), 0) # angle mean 
kappa0 <-c(0.3, 0.1, 0.2) # angle concentration 
anglePar0 <- c(angleMean0, kappa0)

## call to fitting function 
robin_hmm3 <-fitHMM(robin,
                   nbStates = 3,
                   stepPar0 = stepPar0,
                   anglePar0 = anglePar0);beep()

# 4 state model
## Initial parameters for gamma and von Mises distributions
### Step lengths
mu0 <- c(1, 5, 20, 100) # step mean 
sigma0 <- c(1, 1, 5, 20) # step SD 
stepPar0 <- c(mu0, sigma0) 
#### Angle concentrations
angleMean0 <- c(pi, pi, (pi/2), 0) # angle mean 
kappa0 <-c(0.2, 0.2, 0.1, 0.2) # angle concentration 
anglePar0 <- c(angleMean0, kappa0)

## call to fitting function 
robin_hmm4 <-fitHMM(robin,
                   nbStates = 4,
                   stepPar0 = stepPar0,
                   anglePar0 = anglePar0);beep()

```

Run on all birds and compare a 3 and 4 state model

```{r HMM}
# Read in cleaned data prepare moveHMM object
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(DateTime = as.POSIXct(DateTime, 
                                 "%Y-%m-%d %H:%M:%S",tz = "Australia/Melbourne"),
         date = as.Date(DateTime, tz = "Australia/Melbourne")) %>%
  rename(ID = id) %>%
  prepData(type = "UTM", coordNames = c("utm_easting", "utm_northing")) %>%
  filter(step < 500); beep()

# Plot to check
plot(data, compact = TRUE)

# 3 state model
## Initial parameters for gamma and von Mises distributions
### Step lengths
mu0 <- c(0.5, 20, 100) # step mean 
sigma0 <- c(1, 5, 20) # step SD 
zeromass0 <- c(0.1, 0.1, 0.05) # step zero mass
stepPar0 <- c(mu0, sigma0, zeromass0) 
#### Angle concentrations
angleMean0 <- c(pi, (pi/2), 0) # angle mean 
kappa0 <-c(0.3, 0.1, 0.2) # angle concentration 
anglePar0 <- c(angleMean0, kappa0)

## call to fitting function 
hmm3 <-fitHMM(data,
              nbStates = 3,
              stepPar0 = stepPar0,
              anglePar0 = anglePar0);beep()

# 4 state model
## Initial parameters for gamma and von Mises distributions
### Step lengths
mu0 <- c(1, 5, 20, 100) # step mean 
sigma0 <- c(1, 1, 5, 20) # step SD 
stepPar0 <- c(mu0, sigma0) 
#### Angle concentrations
angleMean0 <- c(pi, pi, (pi/2), 0) # angle mean 
kappa0 <-c(0.2, 0.2, 0.1, 0.2) # angle concentration 
anglePar0 <- c(angleMean0, kappa0)

## call to fitting function 
hmm4 <-fitHMM(data,
              nbStates = 4,
              stepPar0 = stepPar0,
              anglePar0 = anglePar0);beep()
```



### Day/night
Since bush stone-curlews are nocturnal, most movement will occur during sunset and sunrise. We can partition the movements by sun position. This is especially useful since most jitter is from stationary roosting points. 

```{r suncalc}
# Read in clean data
data <- read.csv("Processed/GPSdata_clean.csv") %>%
  mutate(DateTime = as.POSIXct(DateTime, 
                                 "%Y-%m-%d %H:%M:%S",tz = "Australia/Melbourne"),
         date = as.Date(DateTime, tz = "Australia/Melbourne"))

# Calculate if time is pre/post dawn/dusk
suntime <- getSunlightTimes(date = unique(data$date),
                            lat = -37.90,
                            lon = 144.43,
                            keep = c("sunrise", "sunset"),
                            tz = "Australia/Melbourne") %>%
  subset(select = -c(lat, lon)) 

# Append to data frame
data_sun <- left_join(data, suntime) %>%
  mutate(tod = ifelse(DateTime>sunrise & DateTime<sunset, "day", "night")) %>%
  relocate(DateTime, .after = tod) %>%
  na.omit()

# Plot to check
ggplot(data_sun)+
  geom_point(aes(lon, lat, colour = tod))

# Marmalade issues?
marma <- filter(data_sun, id == "Marmalade")

marmasp <- vect(marma, geom = c("lon", "lat"), crs = "EPSG:4326")

marma_out <- marma %>%
  mutate(infence = is.related(marmasp, fence, "intersects")) %>%
  filter(infence == FALSE)

# Save day data
data_day <- data_sun %>%
  filter(tod == "day") %>%
  relocate(DateTime, .after = tod)

write.csv(data_day, "Processed/GPSdata_clean_day.csv", row.names = FALSE)

# Save night data
data_night <- data_sun %>%
  filter(tod == "night")

write.csv(data_night, "Processed/GPSdata_clean_night.csv", row.names = FALSE)
```


### Hazard rate

I also want to try using a survival analysis approach, since 'persistence' is a time-to-event response. 

First I plot the survival curve to see if the hazard rate is constant over time (although this is tough with n=13).

```{r hazard}
# Add every day for survival
birds2 <- setdiff(birds, c("Daisy", "Star", "Wobbles"))
survival2 <- data.frame()

for(i in 1:length(birds2)){
  temp <- subset(survival, bird == birds2[i])

  temp2 <- data.frame(date = as_date(temp$start_date:as_date("2024-01-10"))) %>%
  mutate(bird = birds2[i],
         status = ifelse(date<temp$end_date, 1, 0),
         # Convert dates to numeric for use in Surv()
         time = date - as_date(temp$start_date))
  
  survival2 <- rbind(survival2, temp2)
}

# Plot hazard rate 
plot(survfit(Surv(time, status) ~ 1, data = survival2))
```

The hazard rate is approximately linear (i.e., constant), so we should be able to use a Cox proportional hazard model rather than a parametric survival model. 
