---
title: "Bush stone-curlew behaviour and performance analyes"
author: "Shoshana Rapley"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Read in packages
pacman::p_load(beepr, ggmap, glmmTMB, janitor, SDLfilter, solitude, suncalc, tidyverse)

# Google API key for ggmaps
ggmap::register_google(key = "AIzaSyAH3qnqmxDATEluG8lKt2KtnHKLYJc2WaM")

# Background map Orana zones 1 and 2
map <- get_map(c(144.4360, -37.9000), zoom=14, maptype = "satellite")
```

# Introduction

This Markdown documents pre-release assays and post-release survival and establishment metrics for translocated bush stone-curlews at Mt Rothwell, Victoria. 

# Post-release performance

## Data Cleaning
Telemetry are stored on Movebank, and were downloaded for the period 24/10/2022 (one day before the first release) to 17/4/2023 (one day after the 90-day post-release period for the latest released birds).

```{r movebank data}
# List of birds in this translocation
birds <- c("Briar", "Nutmeg", "Star", "Aurora", "Robin", 
           "Iona", "Sage", "Koda", "Brook", "Clover", "Wobbles",
           "Prem", "Rove", "Valentine", "Marmalade")

# Import data from movebank and filter to study birds 
data_raw <- read.csv("Input/movebank_20221019_20230417.csv") %>%
  clean_names() %>% 
  filter(individual_local_identifier %in% birds) %>%
         # Time in posix format
  mutate(DateTime = as.POSIXct(study_local_timestamp, 
                                 "%Y-%m-%d %H:%M:%S",tz = "Australia/Melbourne"),
         date = as.Date(DateTime, tz = "Australia/Melbourne"),
         # Rename columns for SDLfilter
         id = individual_local_identifier,
         lat = location_lat,
         lon = location_long,
         qi = gps_satellite_count); beep()
```

### Speed-based filtering
We cleaned the data of GPS errors using the SDLfilter package, which takes user-defined speed and turning angle thresholds to determine outliers. We calculated the maximum linear and loop speeds for each individual bird, and filtered out the top 1% of speeds based on the distance to and from a point. We also removed points taken with less than 5 satellites (which results in higher GPS error). 

```{r speed filterimng}
# Run SDLfilter for each bird with the speed thresholds calculated per bird üê¢üê¢
data_filt <- data.frame()

for(i in 1:length(birds)){
  subset <- filter(data_raw, id == birds[i])
  vmax <- vmax(subset, prob = 0.98)
  vmaxlp <- vmaxlp(subset, prob = 0.98)
  temp <- ddfilter(subset, 
                   vmax = vmax,
                   vmaxlp = vmaxlp,
                   qi = 5,
                   method = 2)
  data_filt <- rbind(data_filt, temp)
}; beep('coin')

# Plot data cleaning to check effect of filtering
ggmap(map)+
  geom_point(data=data_raw, aes(lon, lat), color="red")+
  geom_point(data=data_filt, aes(lon, lat), color="blue")+
  geom_path(data=data_filt, aes(lon, lat), color="blue")+
  facet_wrap(~id)

# Save to file
write.csv(data_filt, "Processed/gpsdata_filtered_98.csv", row.names = FALSE)
```

For vmax 99: this reduced the data from 590,860 to y i.e., a z% reduction.
For vmax 98: this reduced the data from 590,860 to 572,650 i.e., a 3.08% reduction.

### Accelerometer-based filtering
Additionally we used movement-verified filtering following (Gunner *_et al._* (2022))[https://royalsocietypublishing.org/doi/10.1098/rsif.2021.0692], where the dynamic body acceleration (DBA; sum of tri-axial accelerometer data) should be in line with velocity (calculated from step distance and time from GPS fixes). Stationary points tend to contribute the most to 'jitter', so checking for real movement can help screen for outliers. 

```{r MVF}
# Calculate DBA and VEDBA
data_filt <- read.csv("Processed/gpsdata_filtered_98.csv") %>% # speed-filtered data
          # calculate dba as the sum of absolute value of axial acceleration
  mutate(dba = abs(acceleration_raw_x) + abs(acceleration_raw_y) + abs(acceleration_raw_z),
          # calculate vedba as the sqaure root of the sum of the squared axial values
         vedba = sqrt(acceleration_raw_x^2 + acceleration_raw_y^2 + acceleration_raw_x^2),
          # calculate the mean speed between the forward and back step speed calculations
         mSpeed = ((sSpeed + pSpeed)/2))

# Calculate 5% and 95% values for vedba and mean speed
range <- data.frame(variables = c("vedba", "mSpeed"),
                       lower = c(quantile(data_filt$vedba, 0.05, na.rm = TRUE)[[1]],
                                 quantile(data_filt$mSpeed, 0.05, na.rm = TRUE)[[1]]),
                       upper = c(quantile(data_filt$vedba, 0.95, na.rm = TRUE)[[1]],
                                 quantile(data_filt$mSpeed, 0.95, na.rm = TRUE)[[1]]))
                       
# Filter data based on accelerometer and speed mismatch                  
data_dba <- data_filt %>%
          # flag fixes with zero acceleration, unlikely
  mutate(screen0 = ifelse(vedba == 0, 1, 0),
          # allocate type I errors: VeDBA < lower CI & speed > upper CI (likely resultant from location error)
         screen1 = ifelse(vedba < range[1,2] & mSpeed > range[2,2], 1, 0),
          # allocate type II errors: VeDBA > upper CI & speed < lower (likely resultant from a stationary behaviour),
         screen2 = ifelse(vedba > range[1,3] & mSpeed < range[2,3], 1, 0),
          # combine errors
         error = screen0 + screen1 + screen2) %>%
  filter(error == 0)

# Plot data cleaning to check effect of filtering
ggmap(map)+
    # path
  geom_path(data=data_raw, aes(lon, lat), color="blue")+
  # un-filtered data
  geom_point(data=data_raw, aes(lon, lat), color = "red")+
  # filtered with SDLfilter
  geom_point(data=data_filt, aes(lon, lat), color="blue")+
  # filtered with vedba
  geom_point(data=data_dba, aes(lon, lat), color="yellow")+
  facet_wrap(~id)

# Save to file
write.csv(data_dba, "Processed/gpsdata_filtered_98ac.csv", row.names = FALSE)
```

For vmax 99: this reduced the data from 577,229 rows to 523,057 i.e., a 9.385% reduction. 
For vmax 98: this reduced the data from 572,650 rows to 518,936 i.e., a 9.37% reduction. 

## Distance

We calculated two distance metrics: 
  1) distance from release location
  2) total distance moved daily

```{r distance from release}

# read in clean data
data <- read.csv("movebank_20230510_clean.csv")

# specify the coords of each GPS fix
coords <- as.matrix(cbind(data$utm_easting, data$utm_northing))

# set coords for the release location 
release  <- matrix(c(274423.45, 5801912.85), ncol=2) #(-37.902374, 144.434323)

# calculate distance between the release location and all fixes
dist <- mutate(data, dist_release = distance(coords, as.matrix(release), lonlat=FALSE))

# calculate daily summary statistics
dist_day <- dist %>% group_by(date, id) %>% 
  dplyr::summarise(mean = mean(dist_release),
                   max = max(dist_release),
                   min = min(dist_release)) %>%
  mutate(date = as_date(date)) %>%
  rename(bird = id)

# plot average distance from release site per day
ggplot(dist_day)+
  geom_path(aes(date, mean, group=id, color=id))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Mean daily distance from release location (m)")+
  scale_color_viridis_d()

# and summary stats for whole period
dist_summary <- read.csv("distance_release.csv") %>%
  group_by(bird) %>%
  summarise(mean_daily_dist = mean(mean))

# save results
write.csv(dist_day, "distance_release.csv", row.names = FALSE)
write.csv(dist_summary, "distance_release_mean.csv", row.names = FALSE)

```

Total distance moved daily gives an indication of the activity levels of the bird.

```{r distance moved daily}
# read in clean data
data <- read.csv("movebank_20230510_clean.csv") %>% 
  mutate(date_time = as_date(date_time))

# convert to track
track <- make_track(data, .x = utm_easting,.y= utm_northing,
                    .t= date_time, id = id) %>%
  nest(data = -"id")

# loop through distance per day per bird
dates <- unique(data$date)
dist <- data.frame()

birds <- c("Briar", "Nutmeg", "Star", "Aurora", "Robin", "Daisy",
           "Iona", "Sage", "Koda", "Brook", "Clover", "Wobbles",
           "Prem", "Rove", "Valentine", "Marmalade")

for (i in 1:length(birds)){
  for (j in 1:length(dates)){
    
    t <-  track$data[[i]] %>%
      mutate(date = date(t_)) %>%
      filter(date==dates[j])
    
    if (nrow(t)<2){
      next
    }
    
    out <- data.frame(dist= cum_dist(t)/1000,
                      bird = track$id[[i]],
                      date = dates[j])
    
    print(out)
    
    dist <- rbind.data.frame(dist, out)
    
  }}

dist <- dist %>%
  mutate(date = as_date(date))

# and as an average over the study period
dist_summary <- dist %>% 
  group_by(bird) %>%
  summarise(mean_daily_dist = mean(dist))

# save results
write.csv(dist, "distance_daily.csv", row.names = FALSE)
write.csv(dist_summary, "distance_daily_mean.csv", row.names = FALSE)

# plot
ggplot(dist)+
  geom_path(aes(date, dist, group = bird, color=bird))+
  theme_minimal()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Total daily distance moved (km)")+
  scale_color_viridis_d()
```

Can add the weather to the distance moved graph, to visually inspect any potential correlation. The weather data came from the BOM climate data online
http://www.bom.gov.au/climate/data/ observations were drawn from Avalon Airport {station 087113}.

```{r distance and weather}

# read in weather data, which are stored by month
weather <- list.files(pattern = "IDC") %>%
  map_df(~read_csv(., locale=locale(encoding="latin1"),show_col_types = FALSE)) %>%
  mutate(date = as.Date(Date, tryFormats="%d/%m/%Y"))

# read in distance data
dist <- read.csv("distance_daily.csv")

# plot  
ggplot()+
  geom_ribbon(data=weather, aes(x = date, 
                               ymin=`Minimum temperature (¬∞C)`,
                               ymax=`Maximum temperature (¬∞C)`),
              fill="purple", alpha=0.1) + 
  
  xlim(as.Date("2022-10-25"), NA)+
  geom_path(data=dist, aes(date, dist, color=bird))+
  theme_minimal()+
  scale_color_viridis_d()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  scale_x_date(date_labels = "%b %Y", breaks = "1 month")+
  labs(x= element_blank(), y = "Total daily distance moved (km)")+
  scale_y_continuous(name = "Temperature (¬∞C)",
    sec.axis = sec_axis( trans=~.*1, name="Distance moved (km)"))

```

## Home range

Core 50% KUD and full 90% KUD home range using kernel utilisation density. 

```{r home range}

# read in clean data
data <- read.csv("movebank_20230510_clean.csv")

# convert GPS fixes to spatial points data frame
locs <- SpatialPointsDataFrame(coordinates(
  cbind(data$utm_easting, data$utm_northing)),
  data = dplyr::select(data, id))

# calculate home range 
## 90% kud
hr_90 <-  kernelUD(locs, h="href", grid=100) %>% 
  getverticeshr(percent = 90)

## 50% kud
hr_50 <-  kernelUD(locs, h="href", grid=100) %>% 
  getverticeshr(percent = 50)

# transform to latlon
## 90% kud
proj4string(hr_90) <- CRS("EPSG:32755") #set crs
hr_90ll <- spTransform(hr_90, CRS("EPSG:4326")) %>% #transform to lat lon
  st_as_sf()

## 50% kud
proj4string(hr_50) <- CRS("EPSG:32755") #set crs
hr_50ll <- spTransform(hr_50, CRS("EPSG:4326")) %>% #transform to lat lon
  st_as_sf()

# background map for plotting
map <- get_map(c(144.435738, -37.900544), zoom=15, maptype = "satellite") 

# plot map
## 90% kud
ggmap(map)+
  geom_sf(data=hr_90ll, aes(fill=id), alpha = .7, inherit.aes = FALSE) +
  scale_fill_viridis_d()

## 50% kud
ggmap(map)+
  geom_sf(data=hr_50ll, aes(fill=id), alpha = .7, inherit.aes = FALSE) +
  scale_fill_viridis_d()

# save results
hr <- data.frame(bird = hr_90@data$id,
                 kud90 = hr_90@data$area,
                 kud50 = hr_50@data$area)
write.csv(hr, "hr_area.csv", row.names = FALSE)

```

And daily 90% home range

```{r home range daily}

# read in clean data
data <- read.csv("movebank_20230510_clean.csv") %>%
  mutate(date = as_date(date))

# convert GPS fixes to spatial points data frame
locs <- SpatialPointsDataFrame(coordinates(
  cbind(data$utm_easting, data$utm_northing)),
  data = data)

# loop through birds and days for 90% KUD
birds <- as.character(unique(data$id))
days <- unique(locs[["date"]])

hr_daily <- data.frame()

for (i in 1:length(birds)){
  for (j in 1:length(days)){
    points <- subset(locs, id == birds[i] & date == days[j],
                     select = id)
    if (length(points)<5){
      next
    }
    kud <- kernelUD(points[,1], h="href", grid=1000, extent = 5) %>% 
      getverticeshr(percent = 90) %>%
      st_as_sf() %>%
      mutate(date = days[j])

    print(head(kud, n= 1L))
    
    hr_daily <- rbind.data.frame(hr_daily, kud)
  }}

# extract area per day
hr_area <- st_drop_geometry(hr_daily)
write.csv(hr_area, "hr_daily_area.csv", row.names = TRUE)
    

```

## Sociality

Correlation coefficient

```{r sociality}

# read in clean data
data <- read.csv("movebank_20230510_clean.csv")
data <- data %>%
mutate(date_time = as.POSIXct(date_time))

# set up data in ltraj format for wildlifeDI
data_ltraj <- as.ltraj(xy = data[,c("utm_easting", "utm_northing")],
                       date =  data$date_time,
                       id = data$id,
                       proj4string = CRS("EPSG:32755"))

# run all combinations of bird interactions for dynamic wildlife interactions
list <-combinations(n = 16, r = 2, v = 1:16, repeats.allowed = FALSE)
list1 <- list[,1]
list2 <- list[,2]
interact <- data.frame()

# Parallelized loop with progress bar (give the pc 4 cores leftover)
#cores <- detectCores()
cl <- makeCluster(6)
registerDoParallel(cl)

# Initialize a progressor function before running the loop
library(progressr)
library(future)
plan(multisession)
handlers("txtprogressbar")

calc_interactions <- function(i, list1, list2, data_ltraj) {
  
    prox <- tryCatch({
      data.frame(
        prox = Prox(data_ltraj[list1[i]], data_ltraj[list2[i]], tc=120, dc=50),
        bird1 = attr(data_ltraj[[list1[i]]], "id"),
        bird2 = attr(data_ltraj[[list2[i]]], "id"))
    }, error = function(e) data.frame(prox = NA, bird1 = NA, bird2 = NA))
    
    cr <- tryCatch({
      data.frame(
        cr = Cr(data_ltraj[list1[i]], data_ltraj[list2[i]], tc=120),
        bird1 = attr(data_ltraj[[list1[i]]], "id"),
        bird2 = attr(data_ltraj[[list2[i]]], "id"))
    }, error = function(e) data.frame(cr = NA, bird1 = NA, bird2 = NA))
  
    
    cs <- tryCatch({
      data.frame(
        cs = Cs(data_ltraj[list1[i]], data_ltraj[list2[i]], tc=120),
        bird1 = attr(data_ltraj[[list1[i]]], "id"),
        bird2 = attr(data_ltraj[[list2[i]]], "id"))
    }, error = function(e) data.frame(cs = NA, bird1 = NA, bird2 = NA))
  
      
    iab <- tryCatch({
      data.frame(
        iab = IAB(data_ltraj[list1[i]], data_ltraj[list2[i]], tc=120, dc=50),
        bird1 = attr(data_ltraj[[list1[i]]], "id"),
        bird2 = attr(data_ltraj[[list2[i]]], "id"))
    }, error = function(e) data.frame(iab = NA, bird1 = NA, bird2 = NA))
  
      
    di <- tryCatch({
      data.frame(
        di = DI(data_ltraj[list1[i]], data_ltraj[list2[i]], tc=120),
        bird1 = attr(data_ltraj[[list1[i]]], "id"),
        bird2 = attr(data_ltraj[[list2[i]]], "id"))
    }, error = function(e) data.frame(di = NA, bird1 = NA, bird2 = NA))
  
  
    out <- left_join(prox, cr, by = c("bird1", "bird2")) %>%
      left_join(cs, by = c("bird1", "bird2")) %>%
      left_join(iab, by = c("bird1", "bird2")) %>%
      left_join(di, by = c("bird1", "bird2"))
  
  out
}


with_progress({
  p <- progressor(along = list1)
  
  # Run the loop using future_lapply, with progress updates
  results <- future_lapply(1:length(list1), function(i) {
    p()  # update the progress bar
    calc_interactions(i, list1, list2, data_ltraj)
  })
})

# Combine the results (didn't work because of missmatched rows)
#interact <- do.call(rbind, results) 

# Combine the results
interact <- bind_rows(results)

stopCluster(cl)

# save results
write.csv(interact, "global_interactions.csv", row.names = FALSE)

```

# Modelled response

Here I use the pre-release assays as predictors for the post-release response variables. 

## Predictor variable: latency to reach food

Latency to reach food is how long it took each bird, in seconds, to reach a plate of 
food provided as part of their normal husbandry, measured from when the plate was 
placed on the ground until the bird took the first mouthful. 

First, test latency to reach food as a predictor for survival

```{r m1}

# read in data
m1_data <- left_join(read.csv("survival.csv"), read.csv("latency.csv")) %>%
  clean_names() %>%
  mutate(latency = ifelse(is.na(latency), 21600, latency)) 

m1a_data <- m1_data %>%
  group_by(bird) %>%
  summarise(latency = mean(latency),
            persistence = max(persistence),
            offset = max(offset),
            survived = min(survived))

# m1a: reach food (yes/no) and survival (yes/no)
mosaicplot(table(m1_data$survived, m1_data$reach))

m1a <- chisq.test(table(m1_data$survived, m1_data$reach))
m1a

# m1b: latency to reach food (seconds) and survival (yes/no)
ggplot(m1a_data, aes(x= survived, y=log(latency), group=survived))+
  geom_boxplot()+
  theme_minimal()

yes <- dplyr::filter(m1_data, survived==1)
no <- dplyr::filter(m1_data, survived==0)

t.test(yes$latency, no$latency)

```

Second, test latency to reach food as a predictor for persistence 

```{r m2}
ggplot(m1a_data, aes(x= persistence, y=log(latency)))+
  geom_point()+
  geom_smooth(method=lm)

m2 <- glmmTMB(persistence ~ scale(I(log(latency))) + offset(scale(offset)),
               family = gaussian,
               data = m1_data)
  
summary(m2)

performance(m2)

check_model(m2)

plot(m2)
```

Third, test latency to reach food as a predictor for average distance moved

```{r m3}

# read in data
m3_data <- left_join(read.csv("distance_daily.csv"),
                     clean_names(read.csv("latency.csv")), by = "bird") %>%
  mutate(date.x = (as_date(date.x, format = "%d/%m/%Y"))) %>%
  filter(!bird=="Daisy") %>%
  mutate(latency = ifelse(is.na(latency), 21600, latency),
         ar = as.numeric(factor(as.numeric(date.x))))

m3a_data <- m3_data %>%
  group_by(bird) %>%
  summarise(latency = mean(latency),
            dist = mean(dist))

# visualise distance over time

ggplot(m3a_data, aes(dist, log(latency)))+
  geom_point()+
  geom_smooth(method=lm)+
  theme_minimal()

# m3 latency and distance moved

m3 <- glmmTMB(log(latency) ~ scale(dist),
              family = gaussian,
              data = m3_data_sum)

summary(m3)

```

Fourth, test latency to reach food as a predictor for distance moved from the release site

```{r m4}
# read in data
m4_data <- left_join(read.csv("distance_release.csv"),
                     clean_names(read.csv("latency.csv")), by = "bird")

m4_data_sum <- left_join(read.csv("distance_release_mean.csv"),
                     clean_names(read.csv("latency.csv")), by = "bird")

# m4a: reach food (yes/no) and distance moved from release
ggplot(m4_data_sum, aes(x=reach, y=mean_daily_dist, group=reach))+
  geom_boxplot() +
  theme_minimal()

m4a <- glmmTMB(reach ~ mean + (1| date.x + bird),
               family = binomial,
               data = m4_data)

summary(m4a)

# m4b: latency to reach food and distance moved from release
# NEED TO FIX FOR AUTOCORRELATION AMONG DAYS
ggplot(m4_data_sum, aes(mean_daily_dist, latency))+
  geom_point()+
  geom_smooth(method=lm)+
  theme_minimal()

m4b <- glmmTMB(mean ~ log(latency) + (1|bird),
               data = drop_na(m4_data))

summary(m4b)

```

## Predictor variable: handling response

Handling response is an ordinal variable assigned during processing, on a scale of 1:3,
where 1 is calm/still, 2 is moderate/slight kicking or struggling/soft calling or beak clacking, 
and 3 is extreme/upset/shrieking/growling

First, test handling response as a predictor for survival and persistence

```{r m5}
# read in data
m5_data <- left_join(read.csv("handling.csv"),
                     clean_names(read.csv("survival.csv")), by = "bird") %>%
  filter(type == "Pre") %>%
  na.omit() %>%
  mutate(score = ordered(score)) 

# m5a: handling response and survival
mosaicplot(table(m5_data$survived, m5_data$score))

m5a <- chisq.test(table(m5_data$survived, m5_data$score))
m5a

# m5b: handling response and persistence
ggplot(m5_data, aes(x=score, y=persistence, group=score))+
  geom_boxplot() +
  theme_minimal()

m5b <- glmmTMB(persistence ~ score,
               data = m5_data)
summary(m5b)

```

We split the data into night and day (i.e. moving versus roosting).

```{r day night}
# Calculate if time is pre/post dawn/dusk
suntime <- getSunlightTimes(date = unique(data_raw$date),
                            lat = -37.90,
                            lon = 144.43,
                            keep = c("dawn", "dusk"),
                            tz = "Australia/Melbourne") %>%
  subset(select = -c(lat, lon))

# Append to data frame
data_sun <- left_join(data_raw, suntime) %>%
  mutate(tod = ifelse(time_local>dawn & time_local<dusk, "day", "night"))

# Save day data
data_day <- data_sun %>%
  filter(tod == "day")

write.csv(data_day, "Processed/gps_MR2022_day.csv", row.names = FALSE)

# Save night data
data_night <- data_sun %>%
  filter(tod == "night")

write.csv(data_night, "Processed/gps_MR2022_night.csv", row.names = FALSE)
```

