---
title: "accelerometer filtering"
author: "Shoshana Rapley"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Read in packages
pacman::p_load(beepr, ggmap, glmmTMB, janitor, SDLfilter, solitude, suncalc, tidyverse)

# Google API key for ggmaps
ggmap::register_google(key = "AIzaSyAH3qnqmxDATEluG8lKt2KtnHKLYJc2WaM")

# Background map Orana zones 1 and 2
map <- get_map(c(144.4360, -37.9000), zoom=14, maptype = "satellite")
```

I tried movement-verified filtering following (Gunner *_et al._* (2022))[https://royalsocietypublishing.org/doi/10.1098/rsif.2021.0692], where the dynamic body acceleration (DBA; sum of tri-axial accelerometer data) should be in line with velocity (calculated from step distance and time from GPS fixes). Stationary points tend to contribute the most to 'jitter', so checking for real movement can help screen for outliers. 

However this didn't give as much benefit as I expected, so for now not doing for the entire dataset. I got most value out of the speed-based filtering, and only saw an extra 0.07% reduction in errors on top of the 4% reduction at the first stage. Just leaving code here for future reference. 

```{r MVF}
## read in temp data that I saved for Briar one week filtered data
data_raw <- read.csv("temp_raw.csv")
data_filt <- read.csv("temp_filt.csv")

## or the data for all birds
# List of birds in this translocation
birds <- c("Briar", "Nutmeg", "Star", "Aurora", "Robin", 
           "Iona", "Sage", "Koda", "Brook", "Clover", "Wobbles",
           "Prem", "Rove", "Valentine", "Marmalade")

# Import data from movebank and filter to study birds 
data_raw <- read.csv("Input/movebank_20221019_20230417.csv") %>%
  clean_names() %>% 
  filter(individual_local_identifier %in% birds) %>%
         # Time in posix format
  mutate(DateTime = as.POSIXct(study_local_timestamp, 
                                 "%Y-%m-%d %H:%M:%S",tz = "Australia/Melbourne"),
         date = as.Date(DateTime, tz = "Australia/Melbourne"),
         # Rename columns for SDLfilter
         id = individual_local_identifier,
         lat = location_lat,
         lon = location_long,
         qi = gps_satellite_count); beep('coin')
# filtered
data_filt <- read.csv("Processed/gpsdata_filtered.csv")

## accelerometer filtering
# Calculate DBA and VEDBA
data_dba <- data_filt %>%
          # calculate dba as the sum of absolute value of axial acceleration
  mutate(dba = abs(acceleration_raw_x) + abs(acceleration_raw_y) + abs(acceleration_raw_z),
          # calculate vedba as the sqaure root of the sum of the squared axial values
         vedba = sqrt(acceleration_raw_x^2 + acceleration_raw_y^2 + acceleration_raw_x^2),
          # calculate the mean speed between the forward and back step speed calculations
         mSpeed = ((sSpeed + pSpeed)/2))

# Calculate 5% and 95% values for vedba and mean speed
range <- data.frame(variables = c("vedba", "mSpeed"),
                       lower = c(quantile(data_dba$vedba, 0.05, na.rm = TRUE)[[1]],
                                 quantile(data_dba$mSpeed, 0.05, na.rm = TRUE)[[1]]),
                       upper = c(quantile(data_dba$vedba, 0.95, na.rm = TRUE)[[1]],
                                 quantile(data_dba$mSpeed, 0.95, na.rm = TRUE)[[1]]))
                       
# Filter data based on accelerometer and speed mismatch                  
data_dba_filt <- data_dba %>%
  mutate( # flag fixes with zero acceleration, unlikely
         screen0 = ifelse(dba == 0, 1, 0),
          # allocate type I errors: VeDBA < lower CI & speed > upper CI (likely resultant from locational error)
         screen1 = ifelse(vedba < range[1,2] & mSpeed > range[2,2], 1, 0),
          # allocate type II errors: VeDBA > upper CI & speed < lower (likely resultant from a stationary behaviour),
         screen2 = ifelse(vedba > range[1,3] & mSpeed < range[2,3], 1, 0),
          # combine errors
         error = screen0 + screen1 + screen2) %>%
  # remove NA error values
  filter(!is.na(error))

# plot data cleaning to check visually
ggmap(map)+
  # un-filtered data
  geom_point(data=data_raw, aes(lon, lat), color="red")+
  # path
  geom_path(data=data_raw, aes(lon, lat), color="blue")+
  # flagged not an error
  geom_point(data=filter(data_dba_filt, error == 0), aes(lon, lat), color="blue")+
  # flagged errors
  geom_point(data=filter(data_dba_filt, error > 0), aes(lon, lat), color="yellow")+
  facet_wrap(~id)
```

